{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HGP-SL (Pytorch) BIONETdata.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8zD4c4oPNiOO601savYVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giordamaug/BIONETdatasets/blob/main/notebooks/HGP_SL_(Pytorch)_BIONETdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kux3bRpFEEoM"
      },
      "source": [
        "# Cloning BIONETdatasets repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CemjxolD1na"
      },
      "source": [
        "!git clone http://github.com/giordamaug/BIONETdatasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daN-NtkfEfZw"
      },
      "source": [
        "!pip install -q sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrNDJKSveJNs"
      },
      "source": [
        "!pip install torch==1.7.0\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "!pip install torch-scatter==2.0.5     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse==0.6.8      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster==1.5.8     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv==1.2.0 -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric==1.6.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tesxawOSfuvn"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.io import read_tu_data\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import InMemoryDataset, extract_zip\n",
        "import os\n",
        "import os.path as osp\n",
        "import shutil\n",
        "class MyTUDataset(InMemoryDataset):\n",
        "    r\"\"\"A variety of graph kernel benchmark datasets, *.e.g.* \"IMDB-BINARY\",\n",
        "    \"REDDIT-BINARY\" or \"PROTEINS\", collected from the `TU Dortmund University\n",
        "    <https://chrsmrrs.github.io/datasets>`_.\n",
        "    In addition, this dataset wrapper provides `cleaned dataset versions\n",
        "    <https://github.com/nd7141/graph_datasets>`_ as motivated by the\n",
        "    `\"Understanding Isomorphism Bias in Graph Data Sets\"\n",
        "    <https://arxiv.org/abs/1910.12091>`_ paper, containing only non-isomorphic\n",
        "    graphs.\n",
        "\n",
        "    .. note::\n",
        "        Some datasets may not come with any node labels.\n",
        "        You can then either make use of the argument :obj:`use_node_attr`\n",
        "        to load additional continuous node attributes (if present) or provide\n",
        "        synthetic node features using transforms such as\n",
        "        like :class:`torch_geometric.transforms.Constant` or\n",
        "        :class:`torch_geometric.transforms.OneHotDegree`.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory where the dataset should be saved.\n",
        "        name (string): The `name\n",
        "            <https://chrsmrrs.github.io/datasets/docs/datasets/>`_ of the\n",
        "            dataset.\n",
        "        transform (callable, optional): A function/transform that takes in an\n",
        "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
        "            version. The data object will be transformed before every access.\n",
        "            (default: :obj:`None`)\n",
        "        use_node_attr (bool, optional): If :obj:`True`, the dataset will\n",
        "            contain additional continuous node attributes (if present).\n",
        "            (default: :obj:`False`)\n",
        "        use_edge_attr (bool, optional): If :obj:`True`, the dataset will\n",
        "            contain additional continuous edge attributes (if present).\n",
        "            (default: :obj:`False`)\n",
        "    \"\"\"\n",
        "    def __init__(self, name, path, use_node_attr=False, use_edge_attr=False):\n",
        "        self.name = name\n",
        "        self.path = path\n",
        "        super(MyTUDataset, self).__init__(path, transform=None, pre_transform=None,\n",
        "                                        pre_filter=None)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "        if self.data.x is not None and not use_node_attr:\n",
        "            num_node_attributes = self.num_node_attributes\n",
        "            self.data.x = self.data.x[:, num_node_attributes:]\n",
        "        if self.data.edge_attr is not None and not use_edge_attr:\n",
        "            num_edge_attributes = self.num_edge_attributes\n",
        "            self.data.edge_attr = self.data.edge_attr[:, num_edge_attributes:]\n",
        "\n",
        "\n",
        "    @property\n",
        "    def num_node_labels(self):\n",
        "        if self.data.x is None:\n",
        "            return 0\n",
        "        for i in range(self.data.x.size(1)):\n",
        "            x = self.data.x[:, i:]\n",
        "            if ((x == 0) | (x == 1)).all() and (x.sum(dim=1) == 1).all():\n",
        "                return self.data.x.size(1) - i\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def num_node_attributes(self):\n",
        "        if self.data.x is None:\n",
        "            return 0\n",
        "        return self.data.x.size(1) - self.num_node_labels\n",
        "\n",
        "    @property\n",
        "    def num_edge_labels(self):\n",
        "        if self.data.edge_attr is None:\n",
        "            return 0\n",
        "        for i in range(self.data.edge_attr.size(1)):\n",
        "            if self.data.edge_attr[:, i:].sum() == self.data.edge_attr.size(0):\n",
        "                return self.data.edge_attr.size(1) - i\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def num_edge_attributes(self):\n",
        "        if self.data.edge_attr is None:\n",
        "            return 0\n",
        "        return self.data.edge_attr.size(1) - self.num_edge_labels\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        names = ['A', 'graph_indicator']\n",
        "        return ['{}_{}.txt'.format(self.name, name) for name in names]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'data.pt'\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        self.data, self.slices = read_tu_data(self.path, self.name)\n",
        "\n",
        "        data_list = [self.get(idx) for idx in range(len(self))]\n",
        "        self.data, self.slices = self.collate(data_list)\n",
        "\n",
        "        torch.save((self.data, self.slices), self.processed_paths[0])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({})'.format(self.name, len(self))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-ofwbmSJrpt"
      },
      "source": [
        "# Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eTgDV3DEwyM"
      },
      "source": [
        "#@title select the dataset { form-width: \"30%\" }\n",
        "dataname = \"PROTEINS\" #@param [\"ogbg-molbace\", \"ogbg-molbbbp\", \"KIDNEY\", \"MUTAG\", \"PROTEINS\", \"Mutagenicity\" ]\n",
        "import shutil\n",
        "import os\n",
        "shutil.unpack_archive(f'/content/BIONETdatasets/TUD/{dataname}.zip', '/content')\n",
        "import sys\n",
        "sys.path.append('BIONETdatasets/TUD')\n",
        "dataset = MyTUDataset(dataname, path=dataname, use_node_attr=True, use_edge_attr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHiz_se2jj8J"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeXcGIzPl3Yv"
      },
      "source": [
        "# Apply HGP-SL model on dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHfi2U8DlCvq"
      },
      "source": [
        "### Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfMOKqkkm6Ll"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.pool.topk_pool import topk, filter_adj\n",
        "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
        "from torch_scatter import scatter_add\n",
        "from torch_sparse import spspmm, coalesce\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "from torch_scatter import scatter_add, scatter_max\n",
        "\n",
        "def scatter_sort(x, batch, fill_value=-1e16):\n",
        "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
        "    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
        "\n",
        "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
        "\n",
        "    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
        "    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
        "\n",
        "    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n",
        "    dense_x[index] = x\n",
        "    dense_x = dense_x.view(batch_size, max_num_nodes)\n",
        "\n",
        "    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n",
        "    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n",
        "    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n",
        "\n",
        "    sorted_x = sorted_x.view(-1)\n",
        "    filled_index = sorted_x != fill_value\n",
        "\n",
        "    sorted_x = sorted_x[filled_index]\n",
        "    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n",
        "\n",
        "    return sorted_x, cumsum_sorted_x\n",
        "\n",
        "\n",
        "def _make_ix_like(batch):\n",
        "    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n",
        "    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n",
        "    idx = torch.cat(idx, dim=0)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def _threshold_and_support(x, batch):\n",
        "    \"\"\"Sparsemax building block: compute the threshold\n",
        "    Args:\n",
        "        x: input tensor to apply the sparsemax\n",
        "        batch: group indicators\n",
        "    Returns:\n",
        "        the threshold value\n",
        "    \"\"\"\n",
        "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
        "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
        "\n",
        "    sorted_input, input_cumsum = scatter_sort(x, batch)\n",
        "    input_cumsum = input_cumsum - 1.0\n",
        "    rhos = _make_ix_like(batch).to(x.dtype)\n",
        "    support = rhos * sorted_input > input_cumsum\n",
        "\n",
        "    support_size = scatter_add(support.to(batch.dtype), batch)\n",
        "    # mask invalid index, for example, if batch is not start from 0 or not continuous, it may result in negative index\n",
        "    idx = support_size + cum_num_nodes - 1\n",
        "    mask = idx < 0\n",
        "    idx[mask] = 0\n",
        "    tau = input_cumsum.gather(0, idx)\n",
        "    tau /= support_size.to(x.dtype)\n",
        "\n",
        "    return tau, support_size\n",
        "\n",
        "\n",
        "class SparsemaxFunction(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, batch):\n",
        "        \"\"\"sparsemax: normalizing sparse transform\n",
        "        Parameters:\n",
        "            ctx: context object\n",
        "            x (Tensor): shape (N, )\n",
        "            batch: group indicator\n",
        "        Returns:\n",
        "            output (Tensor): same shape as input\n",
        "        \"\"\"\n",
        "        max_val, _ = scatter_max(x, batch)\n",
        "        x -= max_val[batch]\n",
        "        tau, supp_size = _threshold_and_support(x, batch)\n",
        "        output = torch.clamp(x - tau[batch], min=0)\n",
        "        ctx.save_for_backward(supp_size, output, batch)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        supp_size, output, batch = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[output == 0] = 0\n",
        "\n",
        "        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n",
        "        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n",
        "\n",
        "        return grad_input, None\n",
        "\n",
        "sparsemax = SparsemaxFunction.apply\n",
        "\n",
        "class Sparsemax(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Sparsemax, self).__init__()\n",
        "\n",
        "    def forward(self, x, batch):\n",
        "        return sparsemax(x, batch)\n",
        "\n",
        "class TwoHopNeighborhood(object):\n",
        "    def __call__(self, data):\n",
        "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
        "        n = data.num_nodes\n",
        "\n",
        "        value = edge_index.new_ones((edge_index.size(1),), dtype=torch.float)\n",
        "\n",
        "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n)\n",
        "        value.fill_(0)\n",
        "\n",
        "        edge_index = torch.cat([edge_index, index], dim=1)\n",
        "        if edge_attr is None:\n",
        "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
        "        else:\n",
        "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
        "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
        "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
        "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n)\n",
        "            data.edge_attr = edge_attr\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}()'.format(self.__class__.__name__)\n",
        "\n",
        "\n",
        "class GCN(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
        "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.cached = cached\n",
        "        self.cached_result = None\n",
        "        self.cached_num_edges = None\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
        "        nn.init.xavier_uniform_(self.weight.data)\n",
        "\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "            nn.init.zeros_(self.bias.data)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.cached_result = None\n",
        "        self.cached_num_edges = None\n",
        "\n",
        "    @staticmethod\n",
        "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
        "\n",
        "        row, col = edge_index\n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        x = torch.matmul(x, self.weight)\n",
        "\n",
        "        if self.cached and self.cached_result is not None:\n",
        "            if edge_index.size(1) != self.cached_num_edges:\n",
        "                raise RuntimeError(\n",
        "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
        "\n",
        "        if not self.cached or self.cached_result is None:\n",
        "            self.cached_num_edges = edge_index.size(1)\n",
        "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
        "            self.cached_result = edge_index, norm\n",
        "\n",
        "        edge_index, norm = self.cached_result\n",
        "\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        if self.bias is not None:\n",
        "            aggr_out = aggr_out + self.bias\n",
        "        return aggr_out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n",
        "\n",
        "\n",
        "class NodeInformationScore(MessagePassing):\n",
        "    def __init__(self, improved=False, cached=False, **kwargs):\n",
        "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.improved = improved\n",
        "        self.cached = cached\n",
        "        self.cached_result = None\n",
        "        self.cached_num_edges = None\n",
        "\n",
        "    @staticmethod\n",
        "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
        "        if edge_weight is None:\n",
        "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
        "\n",
        "        row, col = edge_index\n",
        "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
        "        row, col = edge_index\n",
        "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
        "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
        "\n",
        "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        if self.cached and self.cached_result is not None:\n",
        "            if edge_index.size(1) != self.cached_num_edges:\n",
        "                raise RuntimeError(\n",
        "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
        "\n",
        "        if not self.cached or self.cached_result is None:\n",
        "            self.cached_num_edges = edge_index.size(1)\n",
        "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
        "            self.cached_result = edge_index, norm\n",
        "\n",
        "        edge_index, norm = self.cached_result\n",
        "\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    def message(self, x_j, norm):\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "\n",
        "class HGPSLPool(torch.nn.Module):\n",
        "    def __init__(self, in_channels, ratio=0.8, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n",
        "        super(HGPSLPool, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.ratio = ratio\n",
        "        self.sample = sample\n",
        "        self.sparse = sparse\n",
        "        self.sl = sl\n",
        "        self.negative_slop = negative_slop\n",
        "        self.lamb = lamb\n",
        "\n",
        "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att.data)\n",
        "        self.sparse_attention = Sparsemax()\n",
        "        self.neighbor_augment = TwoHopNeighborhood()\n",
        "        self.calc_information_score = NodeInformationScore()\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch=None):\n",
        "        if batch is None:\n",
        "            batch = edge_index.new_zeros(x.size(0))\n",
        "\n",
        "        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n",
        "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
        "\n",
        "        # Graph Pooling\n",
        "        original_x = x\n",
        "        perm = topk(score, self.ratio, batch)\n",
        "        x = x[perm]\n",
        "        batch = batch[perm]\n",
        "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
        "\n",
        "        # Discard structure learning layer, directly return\n",
        "        if self.sl is False:\n",
        "            return x, induced_edge_index, induced_edge_attr, batch\n",
        "\n",
        "        # Structure Learning\n",
        "        if self.sample:\n",
        "            # A fast mode for large graphs.\n",
        "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
        "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
        "            # edge weights between them.\n",
        "            k_hop = 3\n",
        "            if edge_attr is None:\n",
        "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
        "\n",
        "            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "            for _ in range(k_hop - 1):\n",
        "                hop_data = self.neighbor_augment(hop_data)\n",
        "            hop_edge_index = hop_data.edge_index\n",
        "            hop_edge_attr = hop_data.edge_attr\n",
        "            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
        "\n",
        "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
        "            row, col = new_edge_index\n",
        "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
        "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
        "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
        "            adj[row, col] = weights\n",
        "            new_edge_index, weights = dense_to_sparse(adj)\n",
        "            row, col = new_edge_index\n",
        "            if self.sparse:\n",
        "                new_edge_attr = self.sparse_attention(weights, row)\n",
        "            else:\n",
        "                new_edge_attr = softmax(weights, row, x.size(0))\n",
        "            # filter out zero weight edges\n",
        "            adj[row, col] = new_edge_attr\n",
        "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
        "            # release gpu memory\n",
        "            del adj\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
        "            if edge_attr is None:\n",
        "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
        "                                               device=induced_edge_index.device)\n",
        "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
        "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
        "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
        "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
        "            # Construct batch fully connected graph in block diagonal matirx format\n",
        "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
        "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
        "            new_edge_index, _ = dense_to_sparse(adj)\n",
        "            row, col = new_edge_index\n",
        "\n",
        "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
        "            weights = F.leaky_relu(weights, self.negative_slop)\n",
        "            adj[row, col] = weights\n",
        "            induced_row, induced_col = induced_edge_index\n",
        "\n",
        "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
        "            weights = adj[row, col]\n",
        "            if self.sparse:\n",
        "                new_edge_attr = self.sparse_attention(weights, row)\n",
        "            else:\n",
        "                new_edge_attr = softmax(weights, row, x.size(0))\n",
        "            # filter out zero weight edges\n",
        "            adj[row, col] = new_edge_attr\n",
        "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
        "            # release gpu memory\n",
        "            del adj\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return x, new_edge_index, new_edge_attr, batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MM3YR8ilBG4"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "#@title Parameters { form-width: \"30%\" }\n",
        "nhid = 128 #@param {type:\"slider\", min:16, max:128, step:16}\n",
        "pooling_ratio = 0.5 #@param {type:\"number\"}\n",
        "dropout_ratio = 0.0 #@param {type:\"number\"}\n",
        "sample_neighbor = True #@param {type:\"boolean\"}\n",
        "sparse_attention = True #@param {type:\"boolean\"}\n",
        "structure_learning = True #@param {type:\"boolean\"}\n",
        "lamb = 1.0 #@param {type:\"number\"}\n",
        "learning_rate = 0.001  #@param {type:\"number\"}\n",
        "weight_decay = 0.001  #@param {type:\"number\"}\n",
        "epochs = 200 #@param {type:\"slider\", min:0, max:1000, step:20}\n",
        "patience = 100 #@param {type:\"slider\", min:0, max:500, step:20}\n",
        "batch_size = 512  #@param {type:\"slider\", min:1, max:512, step:1}\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.num_features = num_features\n",
        "        self.nhid = nhid\n",
        "        self.num_classes = num_classes\n",
        "        self.pooling_ratio = pooling_ratio\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.sample = sample_neighbor\n",
        "        self.sparse = sparse_attention\n",
        "        self.sl = structure_learning\n",
        "        self.lamb = lamb\n",
        "\n",
        "        self.conv1 = GCNConv(self.num_features, self.nhid)\n",
        "        self.conv2 = GCN(self.nhid, self.nhid)\n",
        "        self.conv3 = GCN(self.nhid, self.nhid)\n",
        "\n",
        "        self.pool1 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
        "        self.pool2 = HGPSLPool(self.nhid, self.pooling_ratio, self.sample, self.sparse, self.sl, self.lamb)\n",
        "\n",
        "        self.lin1 = torch.nn.Linear(self.nhid * 2, self.nhid)\n",
        "        self.lin2 = torch.nn.Linear(self.nhid, self.nhid // 2)\n",
        "        self.lin3 = torch.nn.Linear(self.nhid // 2, self.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        edge_attr = None\n",
        "\n",
        "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
        "        x, edge_index, edge_attr, batch = self.pool1(x, edge_index, edge_attr, batch)\n",
        "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
        "        x, edge_index, edge_attr, batch = self.pool2(x, edge_index, edge_attr, batch)\n",
        "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
        "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
        "\n",
        "        x = F.relu(x1) + F.relu(x2) + F.relu(x3)\n",
        "\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
        "        x = F.relu(self.lin2(x))\n",
        "        x = F.dropout(x, p=self.dropout_ratio, training=self.training)\n",
        "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06ulxrc2A2fG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNwlXkHoJvID"
      },
      "source": [
        "\"\"\"\n",
        "This example shows how to perform graph classification with a simple Graph\n",
        "Isomorphism Network.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tqdm.notebook as tq\n",
        "import time\n",
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "#@title Validation { form-width: \"30%\" }\n",
        "folds = 5  #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "verbose = True #@param {type:\"boolean\"}\n",
        "seed = 42 #@param {type:\"number\"}\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Cross Validation loop\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "targets = dataset.data.y\n",
        "sp = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
        "results = []\n",
        "for idx_tr, idx_te in tq.tqdm(list(sp.split(dataset, targets)), desc=\"fold: \"):\n",
        "  dataset_tr, dataset_te = dataset[idx_tr.tolist()], dataset[idx_te.tolist()]\n",
        "\n",
        "  loader_tr = DataLoader(dataset_tr, batch_size=batch_size)\n",
        "  loader_te = DataLoader(dataset_te, batch_size=batch_size)\n",
        "\n",
        "  model = Model(dataset.num_features, dataset.num_classes).to('cuda:0')\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  def train():\n",
        "    min_loss = 1e10\n",
        "    patience_cnt = 0\n",
        "    val_loss_values = []\n",
        "    best_epoch = 0\n",
        "\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        loss_train = 0.0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(loader_tr):\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to('cuda:0')\n",
        "            out = model(data)\n",
        "            loss = F.nll_loss(out, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_train += loss.item()\n",
        "            pred = out.max(dim=1)[1]\n",
        "            correct += pred.eq(data.y).sum().item()\n",
        "        acc_train = correct / len(loader_tr.dataset)\n",
        "        if verbose: print('Epoch: {:04d}'.format(epoch + 1), 'loss_train: {:.6f}'.format(loss_train),\n",
        "              'acc_train: {:.6f}'.format(acc_train))\n",
        "    return\n",
        "\n",
        "\n",
        "  def compute_test(loader):\n",
        "    model.eval()\n",
        "    correct = 0.0\n",
        "    loss_test = 0.0\n",
        "    for data in loader:\n",
        "        data = data.to('cuda')\n",
        "        out = model(data)\n",
        "        pred = out.max(dim=1)[1]\n",
        "        correct += pred.eq(data.y).sum().item()\n",
        "        loss_test += F.nll_loss(out, data.y).item()\n",
        "    return correct / len(loader.dataset), loss_test\n",
        "\n",
        "  train()\n",
        "  test_acc, test_loss = compute_test(loader_te)\n",
        "  results.append(\n",
        "        (\n",
        "            test_loss,\n",
        "            test_acc,\n",
        "        )\n",
        "    )\n",
        "  if verbose: print(\"Done. Test loss: {}. Test acc: {}\".format(test_loss, test_acc))\n",
        "# Timing\n",
        "temp = time.time() - start\n",
        "hours = temp//3600\n",
        "temp = temp - 3600*hours\n",
        "minutes = temp//60\n",
        "seconds = temp - 60*minutes\n",
        "expired = '%d:%d:%d' %(hours,minutes,seconds)\n",
        "print(\"Done. Test loss: {}. Test acc: {}\".format(*np.mean(results, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woak_b_95l0h"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0-V-LCXmIIk"
      },
      "source": [
        "# Save the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anuEdalOPK-w"
      },
      "source": [
        "method = 'HGP-SL'\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "path = f'{method}_results.csv'\n",
        "if not os.path.exists(path):\n",
        "  dfres = pd.DataFrame(columns=['dataset', 'loss', 'acc', 'folds', 'seed', 'lr', 'epochs', 'batch_size', 'channels', 'date', 'elapsed'])\n",
        "  dfres.to_csv(path, index=False)\n",
        "dfres = pd.read_csv(path)\n",
        "dfres = dfres.append({'dataset': dataname,\n",
        "                      'loss': np.mean(results, 0)[0],\n",
        "                      'acc' : np.mean(results, 0)[1], \n",
        "                      'folds' : folds,\n",
        "                      'seed' : seed, \n",
        "                      'lr' : learning_rate, 'epochs' : epochs, 'batch_size' : batch_size, 'channels' : nhid, \n",
        "                      'date': datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"),\n",
        "                      'elapsed': expired\n",
        "                      }, ignore_index=True)\n",
        "dfres.to_csv(path, index=False)\n",
        "dfres"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}