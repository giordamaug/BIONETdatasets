{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5af2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "import sys,os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ebeba6",
   "metadata": {
    "id": "LG8i_GOyUlxv"
   },
   "source": [
    "# Load the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f37301d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "85171665569446b59009412fdb03a976",
      "55fb64eb3cfa407fbbfaabace9d2304f",
      "fe46446230104fc881cf610bf414f470",
      "7b65c7e37a1345b185c4009d595cc515",
      "344c7fe5b7054e9a8723932d4a10d503",
      "229ca0dae39d442989544aa5ffa74550",
      "3ca87edafe3a4ee2859d30decd52baf3",
      "6285e09cee044b9893e1f30740f2f49d",
      "3bd5b98d001648dc8080334a7c33af44",
      "42b02fcf2b7648ada6bf9f24c3d60daa",
      "a188459911ca4908b6639fce694dadc0"
     ]
    },
    "id": "G00aOfMeUp9A",
    "outputId": "85457c7a-e182-4597-a23b-50107e60338c"
   },
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "if \"wrappers.spektral_wrapper\" in sys.modules:\n",
    "    del sys.modules[\"wrappers.spektral_wrapper\"]\n",
    "\n",
    "from wrappers.spektral_wrapper import MyGMLDataset, MyTUDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acabff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 319.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "Successfully loaded Mutag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ataset = MyGMLDataset('YENDIK', '../../GraphML/datasets')\n",
    "dataset2 = MyGMLDataset('Mutag', '../../../CDS-GROUP/GE METANET/', upto=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9345764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0]]),\n",
       " memoryview)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2[0].x.shape, dataset2[0].e.shape,dataset2[0].a.shape,dataset2.n_labels,dataset2.n_node_features\n",
    "type(dataset2[0].a),dataset2[0].a, type(dataset2[0].a.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51bc0a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(scipy.sparse.csr.csr_matrix,\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.]),\n",
       " numpy.ndarray)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].x.shape, dataset[0].e.shape,dataset[0].a.shape,dataset.n_labels,dataset.n_node_features\n",
    "type(dataset[0].a),dataset[0].a.data, type(dataset[0].a.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59bfe582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded MUTAG.\n"
     ]
    }
   ],
   "source": [
    "dataset = MyTUDataset('MUTAG', 'MUTAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08042649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2,), (3,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2[0].y.shape, dataset[0].y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30a7d7",
   "metadata": {},
   "source": [
    "# Apply GIN-0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84e181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features 82\n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "Ep. 1 - Loss: 1.042484164237976. Acc: 0.4686192572116852\n",
      "Ep. 2 - Loss: 1.0100605487823486. Acc: 0.5313807725906372\n",
      "Ep. 3 - Loss: 1.0226746797561646. Acc: 0.5271966457366943\n",
      "Ep. 4 - Loss: 1.0115495920181274. Acc: 0.5271966457366943\n",
      "Ep. 5 - Loss: 1.0093621015548706. Acc: 0.5355648398399353\n",
      "Ep. 6 - Loss: 1.0282316207885742. Acc: 0.5313807725906372\n",
      "Ep. 7 - Loss: 1.013115406036377. Acc: 0.5313807725906372\n",
      "Ep. 8 - Loss: 1.0087506771087646. Acc: 0.5313807725906372\n",
      "Ep. 9 - Loss: 1.0152674913406372. Acc: 0.5313807725906372\n",
      "Ep. 10 - Loss: 0.9925516843795776. Acc: 0.5313807725906372\n",
      "Ep. 11 - Loss: 1.0069535970687866. Acc: 0.5313807725906372\n",
      "Ep. 12 - Loss: 1.0134238004684448. Acc: 0.5313807725906372\n",
      "Ep. 13 - Loss: 1.0238357782363892. Acc: 0.5313807725906372\n",
      "Ep. 14 - Loss: 1.001293420791626. Acc: 0.5313807725906372\n",
      "Ep. 15 - Loss: 1.0106167793273926. Acc: 0.5313807725906372\n",
      "Ep. 16 - Loss: 1.0015588998794556. Acc: 0.5313807725906372\n",
      "Ep. 17 - Loss: 1.0166993141174316. Acc: 0.5313807725906372\n",
      "Ep. 18 - Loss: 0.9928023219108582. Acc: 0.5313807725906372\n",
      "Ep. 19 - Loss: 1.0033044815063477. Acc: 0.5313807725906372\n",
      "Ep. 20 - Loss: 1.0126662254333496. Acc: 0.5313807725906372\n",
      "Ep. 21 - Loss: 1.0073446035385132. Acc: 0.5313807725906372\n",
      "Ep. 22 - Loss: 1.0064928531646729. Acc: 0.5313807725906372\n",
      "Ep. 23 - Loss: 1.0032578706741333. Acc: 0.5313807725906372\n",
      "Ep. 24 - Loss: 1.0051580667495728. Acc: 0.5313807725906372\n",
      "Ep. 25 - Loss: 1.0100014209747314. Acc: 0.5313807725906372\n",
      "Ep. 26 - Loss: 0.99826580286026. Acc: 0.5313807725906372\n",
      "Ep. 27 - Loss: 1.0098876953125. Acc: 0.5313807725906372\n",
      "Ep. 28 - Loss: 1.0150049924850464. Acc: 0.5313807725906372\n",
      "Ep. 29 - Loss: 1.006005048751831. Acc: 0.5313807725906372\n",
      "Ep. 30 - Loss: 0.998130738735199. Acc: 0.5313807725906372\n",
      "Ep. 31 - Loss: 1.0071173906326294. Acc: 0.5313807725906372\n",
      "Ep. 32 - Loss: 1.0067495107650757. Acc: 0.5313807725906372\n",
      "Ep. 33 - Loss: 1.004485011100769. Acc: 0.5313807725906372\n",
      "Ep. 34 - Loss: 1.0051710605621338. Acc: 0.5313807725906372\n",
      "Ep. 35 - Loss: 0.9981626868247986. Acc: 0.5313807725906372\n",
      "Ep. 36 - Loss: 0.9966866374015808. Acc: 0.5313807725906372\n",
      "Ep. 37 - Loss: 1.0089442729949951. Acc: 0.5313807725906372\n",
      "Ep. 38 - Loss: 0.9974705576896667. Acc: 0.5313807725906372\n",
      "Ep. 39 - Loss: 1.0078539848327637. Acc: 0.5313807725906372\n",
      "Ep. 40 - Loss: 1.0021039247512817. Acc: 0.5313807725906372\n",
      "Ep. 41 - Loss: 1.0037368535995483. Acc: 0.5313807725906372\n",
      "Ep. 42 - Loss: 1.0047674179077148. Acc: 0.5313807725906372\n",
      "Ep. 43 - Loss: 0.9986716508865356. Acc: 0.5313807725906372\n",
      "Ep. 44 - Loss: 0.999935507774353. Acc: 0.5313807725906372\n",
      "Ep. 45 - Loss: 1.0083624124526978. Acc: 0.5313807725906372\n",
      "Ep. 46 - Loss: 1.0005395412445068. Acc: 0.5313807725906372\n",
      "Ep. 47 - Loss: 1.0074824094772339. Acc: 0.5313807725906372\n",
      "Ep. 48 - Loss: 1.0017077922821045. Acc: 0.5313807725906372\n",
      "Ep. 49 - Loss: 1.0056484937667847. Acc: 0.5313807725906372\n",
      "Ep. 50 - Loss: 1.0022104978561401. Acc: 0.5313807725906372\n",
      "Ep. 51 - Loss: 1.0012197494506836. Acc: 0.5313807725906372\n",
      "Ep. 52 - Loss: 1.001006007194519. Acc: 0.5313807725906372\n",
      "Ep. 53 - Loss: 1.0001798868179321. Acc: 0.5313807725906372\n",
      "Ep. 54 - Loss: 1.004941701889038. Acc: 0.5313807725906372\n",
      "Ep. 55 - Loss: 1.0014203786849976. Acc: 0.5313807725906372\n",
      "Ep. 56 - Loss: 0.9974900484085083. Acc: 0.5313807725906372\n",
      "Ep. 57 - Loss: 1.006733775138855. Acc: 0.5313807725906372\n",
      "Ep. 58 - Loss: 0.9943850040435791. Acc: 0.5313807725906372\n",
      "Ep. 59 - Loss: 1.0031898021697998. Acc: 0.5313807725906372\n",
      "Ep. 60 - Loss: 0.9967364072799683. Acc: 0.5313807725906372\n",
      "Ep. 61 - Loss: 0.992988646030426. Acc: 0.5313807725906372\n",
      "Ep. 62 - Loss: 0.9958907961845398. Acc: 0.5313807725906372\n",
      "Ep. 63 - Loss: 0.9998894333839417. Acc: 0.5313807725906372\n",
      "Ep. 64 - Loss: 1.0041046142578125. Acc: 0.5313807725906372\n",
      "Ep. 65 - Loss: 0.9979516863822937. Acc: 0.5313807725906372\n",
      "Ep. 66 - Loss: 0.9959174394607544. Acc: 0.5313807725906372\n",
      "Ep. 67 - Loss: 0.9959692358970642. Acc: 0.5313807725906372\n",
      "Ep. 68 - Loss: 1.0033764839172363. Acc: 0.5313807725906372\n",
      "Ep. 69 - Loss: 0.9989364147186279. Acc: 0.5313807725906372\n",
      "Ep. 70 - Loss: 1.0039128065109253. Acc: 0.5313807725906372\n",
      "Ep. 71 - Loss: 1.000772476196289. Acc: 0.5313807725906372\n",
      "Ep. 72 - Loss: 1.0075829029083252. Acc: 0.5313807725906372\n",
      "Ep. 73 - Loss: 1.004041314125061. Acc: 0.5313807725906372\n",
      "Ep. 74 - Loss: 1.0005497932434082. Acc: 0.5313807725906372\n",
      "Ep. 75 - Loss: 0.9999881386756897. Acc: 0.5313807725906372\n",
      "Ep. 76 - Loss: 1.0028706789016724. Acc: 0.5313807725906372\n",
      "Ep. 77 - Loss: 1.0036184787750244. Acc: 0.5313807725906372\n",
      "Ep. 78 - Loss: 1.000416874885559. Acc: 0.5313807725906372\n",
      "Ep. 79 - Loss: 0.9993076324462891. Acc: 0.5313807725906372\n",
      "Ep. 80 - Loss: 1.002411127090454. Acc: 0.5313807725906372\n",
      "Ep. 81 - Loss: 1.0001657009124756. Acc: 0.5313807725906372\n",
      "Ep. 82 - Loss: 0.9993469715118408. Acc: 0.5313807725906372\n",
      "Ep. 83 - Loss: 1.0013318061828613. Acc: 0.5313807725906372\n",
      "Ep. 84 - Loss: 1.0055081844329834. Acc: 0.5313807725906372\n",
      "Ep. 85 - Loss: 1.0018246173858643. Acc: 0.5313807725906372\n",
      "Ep. 86 - Loss: 0.997248649597168. Acc: 0.5313807725906372\n",
      "Ep. 87 - Loss: 1.0018500089645386. Acc: 0.5313807725906372\n",
      "Ep. 88 - Loss: 0.998999834060669. Acc: 0.5313807725906372\n",
      "Ep. 89 - Loss: 0.9989551901817322. Acc: 0.5313807725906372\n",
      "Ep. 90 - Loss: 1.0023908615112305. Acc: 0.5313807725906372\n",
      "Ep. 91 - Loss: 1.0066304206848145. Acc: 0.5313807725906372\n",
      "Ep. 92 - Loss: 1.0003045797348022. Acc: 0.5313807725906372\n",
      "Ep. 93 - Loss: 1.00149667263031. Acc: 0.5313807725906372\n",
      "Ep. 94 - Loss: 0.9972032904624939. Acc: 0.5313807725906372\n",
      "Ep. 95 - Loss: 1.001869559288025. Acc: 0.5313807725906372\n",
      "Ep. 96 - Loss: 0.9989689588546753. Acc: 0.5313807725906372\n",
      "Ep. 97 - Loss: 0.9979290962219238. Acc: 0.5313807725906372\n",
      "Ep. 98 - Loss: 0.993560254573822. Acc: 0.5313807725906372\n",
      "Ep. 99 - Loss: 0.9978395104408264. Acc: 0.5313807725906372\n",
      "Ep. 100 - Loss: 0.9974645376205444. Acc: 0.5313807725906372\n",
      "Ep. 101 - Loss: 0.9985886216163635. Acc: 0.5313807725906372\n",
      "Ep. 102 - Loss: 0.9976651668548584. Acc: 0.5313807725906372\n",
      "Ep. 103 - Loss: 1.0018415451049805. Acc: 0.5313807725906372\n",
      "Ep. 104 - Loss: 0.9996480941772461. Acc: 0.5313807725906372\n",
      "Ep. 105 - Loss: 0.9998141527175903. Acc: 0.5313807725906372\n",
      "Ep. 106 - Loss: 1.0006619691848755. Acc: 0.5313807725906372\n",
      "Ep. 107 - Loss: 1.0010417699813843. Acc: 0.5313807725906372\n",
      "Ep. 108 - Loss: 0.9980039596557617. Acc: 0.5313807725906372\n",
      "Ep. 109 - Loss: 0.9960319995880127. Acc: 0.5313807725906372\n",
      "Ep. 110 - Loss: 1.0017375946044922. Acc: 0.5313807725906372\n",
      "Ep. 111 - Loss: 0.9994741678237915. Acc: 0.5313807725906372\n",
      "Ep. 112 - Loss: 0.9997404217720032. Acc: 0.5313807725906372\n",
      "Ep. 113 - Loss: 1.0011155605316162. Acc: 0.5313807725906372\n",
      "Ep. 114 - Loss: 0.999199628829956. Acc: 0.5313807725906372\n",
      "Ep. 115 - Loss: 1.000469446182251. Acc: 0.5313807725906372\n",
      "Ep. 116 - Loss: 0.9958233833312988. Acc: 0.5313807725906372\n",
      "Ep. 117 - Loss: 0.9944802522659302. Acc: 0.5313807725906372\n",
      "Ep. 118 - Loss: 0.9981706738471985. Acc: 0.5313807725906372\n",
      "Ep. 119 - Loss: 0.9987322688102722. Acc: 0.5313807725906372\n",
      "Ep. 120 - Loss: 1.000940203666687. Acc: 0.5313807725906372\n",
      "Ep. 121 - Loss: 0.9953709244728088. Acc: 0.5313807725906372\n",
      "Ep. 122 - Loss: 1.001251459121704. Acc: 0.5313807725906372\n",
      "Ep. 123 - Loss: 0.9980819821357727. Acc: 0.5313807725906372\n",
      "Ep. 124 - Loss: 0.9975081086158752. Acc: 0.5313807725906372\n",
      "Ep. 125 - Loss: 0.998362123966217. Acc: 0.5313807725906372\n",
      "Ep. 126 - Loss: 0.997908353805542. Acc: 0.5313807725906372\n",
      "Ep. 127 - Loss: 1.001109004020691. Acc: 0.5313807725906372\n",
      "Ep. 128 - Loss: 0.9984953999519348. Acc: 0.5313807725906372\n",
      "Ep. 129 - Loss: 0.9948154091835022. Acc: 0.5313807725906372\n",
      "Ep. 130 - Loss: 1.0009993314743042. Acc: 0.5313807725906372\n",
      "Ep. 131 - Loss: 0.9991227388381958. Acc: 0.5313807725906372\n",
      "Ep. 132 - Loss: 0.997710108757019. Acc: 0.5313807725906372\n",
      "Ep. 133 - Loss: 0.9988672137260437. Acc: 0.5313807725906372\n",
      "Ep. 134 - Loss: 0.9982503056526184. Acc: 0.5313807725906372\n",
      "Ep. 135 - Loss: 0.9992843866348267. Acc: 0.5313807725906372\n",
      "Ep. 136 - Loss: 0.9972187280654907. Acc: 0.5313807725906372\n",
      "Ep. 137 - Loss: 0.9970014691352844. Acc: 0.5313807725906372\n",
      "Ep. 138 - Loss: 0.9995402693748474. Acc: 0.5313807725906372\n",
      "Ep. 139 - Loss: 0.9986136555671692. Acc: 0.5313807725906372\n",
      "Ep. 140 - Loss: 0.998749315738678. Acc: 0.5313807725906372\n",
      "Ep. 141 - Loss: 0.9980372786521912. Acc: 0.5313807725906372\n",
      "Ep. 142 - Loss: 0.9982221722602844. Acc: 0.5313807725906372\n",
      "Ep. 143 - Loss: 1.0045950412750244. Acc: 0.5230125784873962\n",
      "Ep. 144 - Loss: 1.0000100135803223. Acc: 0.5313807725906372\n",
      "Ep. 145 - Loss: 1.0025871992111206. Acc: 0.5313807725906372\n",
      "Ep. 146 - Loss: 1.0013774633407593. Acc: 0.5313807725906372\n",
      "Ep. 147 - Loss: 0.9983159899711609. Acc: 0.5313807725906372\n",
      "Ep. 148 - Loss: 0.9945865273475647. Acc: 0.5313807725906372\n",
      "Ep. 149 - Loss: 1.0021270513534546. Acc: 0.5313807725906372\n",
      "Ep. 150 - Loss: 0.9984831213951111. Acc: 0.5313807725906372\n",
      "Ep. 151 - Loss: 0.9985010027885437. Acc: 0.5313807725906372\n",
      "Ep. 152 - Loss: 0.9992790222167969. Acc: 0.5313807725906372\n",
      "Ep. 153 - Loss: 0.9963827133178711. Acc: 0.5313807725906372\n",
      "Ep. 154 - Loss: 0.9977749586105347. Acc: 0.5313807725906372\n",
      "Ep. 155 - Loss: 0.9977097511291504. Acc: 0.5313807725906372\n",
      "Ep. 156 - Loss: 0.9968460202217102. Acc: 0.5313807725906372\n",
      "Ep. 157 - Loss: 0.9970550537109375. Acc: 0.5313807725906372\n",
      "Ep. 158 - Loss: 0.9964704513549805. Acc: 0.5313807725906372\n",
      "Ep. 159 - Loss: 0.9967532157897949. Acc: 0.5313807725906372\n",
      "Ep. 160 - Loss: 1.0044759511947632. Acc: 0.5313807725906372\n",
      "Ep. 161 - Loss: 0.9982497692108154. Acc: 0.5313807725906372\n",
      "Ep. 162 - Loss: 0.9999203681945801. Acc: 0.5313807725906372\n",
      "Ep. 163 - Loss: 0.998248815536499. Acc: 0.5313807725906372\n",
      "Ep. 164 - Loss: 0.9997937679290771. Acc: 0.5313807725906372\n",
      "Ep. 165 - Loss: 0.999832272529602. Acc: 0.5313807725906372\n",
      "Ep. 166 - Loss: 0.9967917203903198. Acc: 0.5313807725906372\n",
      "Ep. 167 - Loss: 0.9973223805427551. Acc: 0.5313807725906372\n",
      "Ep. 168 - Loss: 0.9973520636558533. Acc: 0.5313807725906372\n",
      "Ep. 169 - Loss: 0.9978720545768738. Acc: 0.5313807725906372\n",
      "Ep. 170 - Loss: 0.998927116394043. Acc: 0.5313807725906372\n",
      "Ep. 171 - Loss: 0.9974783658981323. Acc: 0.5313807725906372\n",
      "Ep. 172 - Loss: 1.0025792121887207. Acc: 0.5313807725906372\n",
      "Ep. 173 - Loss: 0.9993367195129395. Acc: 0.5313807725906372\n",
      "Ep. 174 - Loss: 0.9998427033424377. Acc: 0.5313807725906372\n",
      "Ep. 175 - Loss: 0.998458206653595. Acc: 0.5313807725906372\n",
      "Ep. 176 - Loss: 0.9976122975349426. Acc: 0.5313807725906372\n",
      "Ep. 177 - Loss: 0.9981446862220764. Acc: 0.5313807725906372\n",
      "Ep. 178 - Loss: 0.9961310625076294. Acc: 0.5313807725906372\n",
      "Ep. 179 - Loss: 0.9987741708755493. Acc: 0.5313807725906372\n",
      "Ep. 180 - Loss: 0.9980380535125732. Acc: 0.5313807725906372\n",
      "Ep. 181 - Loss: 0.9962780475616455. Acc: 0.5313807725906372\n",
      "Ep. 182 - Loss: 0.9988124370574951. Acc: 0.5313807725906372\n",
      "Ep. 183 - Loss: 0.9988258481025696. Acc: 0.5313807725906372\n",
      "Ep. 184 - Loss: 0.9971200823783875. Acc: 0.5313807725906372\n",
      "Ep. 185 - Loss: 0.9958310127258301. Acc: 0.5313807725906372\n",
      "Ep. 186 - Loss: 1.0028387308120728. Acc: 0.5313807725906372\n",
      "Ep. 187 - Loss: 0.9987764358520508. Acc: 0.5313807725906372\n",
      "Ep. 188 - Loss: 0.9987349510192871. Acc: 0.5313807725906372\n",
      "Ep. 189 - Loss: 0.9978517293930054. Acc: 0.5313807725906372\n",
      "Ep. 190 - Loss: 0.997646689414978. Acc: 0.5313807725906372\n",
      "Ep. 191 - Loss: 0.9971465468406677. Acc: 0.5313807725906372\n",
      "Ep. 192 - Loss: 0.9978366494178772. Acc: 0.5313807725906372\n",
      "Ep. 193 - Loss: 0.9976063370704651. Acc: 0.5313807725906372\n",
      "Ep. 194 - Loss: 0.9973658919334412. Acc: 0.5313807725906372\n",
      "Ep. 195 - Loss: 0.9973770380020142. Acc: 0.5313807725906372\n",
      "Ep. 196 - Loss: 0.9974000453948975. Acc: 0.5313807725906372\n",
      "Ep. 197 - Loss: 0.9975641965866089. Acc: 0.5313807725906372\n",
      "Ep. 198 - Loss: 0.997258722782135. Acc: 0.5313807725906372\n",
      "Ep. 199 - Loss: 0.9973095655441284. Acc: 0.5313807725906372\n",
      "Ep. 200 - Loss: 0.9971517324447632. Acc: 0.5313807725906372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold:  20%|██        | 1/5 [09:47<39:08, 587.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Test loss: 0.9950946569442749. Test acc: 0.5333333611488342\n",
      "Ep. 1 - Loss: 1.0374271869659424. Acc: 0.5188284516334534\n",
      "Ep. 2 - Loss: 1.0112583637237549. Acc: 0.5230125784873962\n",
      "Ep. 3 - Loss: 1.0165907144546509. Acc: 0.5313807725906372\n",
      "Ep. 4 - Loss: 1.0059751272201538. Acc: 0.5313807725906372\n",
      "Ep. 5 - Loss: 1.0087835788726807. Acc: 0.5313807725906372\n",
      "Ep. 6 - Loss: 1.0250953435897827. Acc: 0.5313807725906372\n",
      "Ep. 7 - Loss: 1.0175304412841797. Acc: 0.5313807725906372\n",
      "Ep. 8 - Loss: 1.0130006074905396. Acc: 0.5313807725906372\n",
      "Ep. 9 - Loss: 1.002454400062561. Acc: 0.5313807725906372\n",
      "Ep. 10 - Loss: 1.0103925466537476. Acc: 0.5313807725906372\n",
      "Ep. 11 - Loss: 1.0048426389694214. Acc: 0.5313807725906372\n",
      "Ep. 12 - Loss: 1.0143581628799438. Acc: 0.5313807725906372\n",
      "Ep. 13 - Loss: 1.0127204656600952. Acc: 0.5313807725906372\n",
      "Ep. 14 - Loss: 1.0107499361038208. Acc: 0.5313807725906372\n",
      "Ep. 15 - Loss: 1.005923867225647. Acc: 0.5313807725906372\n",
      "Ep. 16 - Loss: 1.0109561681747437. Acc: 0.5313807725906372\n",
      "Ep. 17 - Loss: 1.0073961019515991. Acc: 0.5313807725906372\n",
      "Ep. 18 - Loss: 1.0094548463821411. Acc: 0.5313807725906372\n",
      "Ep. 19 - Loss: 0.9982543587684631. Acc: 0.5313807725906372\n",
      "Ep. 20 - Loss: 1.0096522569656372. Acc: 0.5313807725906372\n",
      "Ep. 21 - Loss: 1.0049240589141846. Acc: 0.5313807725906372\n",
      "Ep. 22 - Loss: 0.998027503490448. Acc: 0.5313807725906372\n",
      "Ep. 23 - Loss: 0.9983486533164978. Acc: 0.5313807725906372\n",
      "Ep. 24 - Loss: 1.0068702697753906. Acc: 0.5313807725906372\n",
      "Ep. 25 - Loss: 1.0060654878616333. Acc: 0.5313807725906372\n",
      "Ep. 26 - Loss: 1.0072128772735596. Acc: 0.5313807725906372\n",
      "Ep. 27 - Loss: 1.0009090900421143. Acc: 0.5313807725906372\n",
      "Ep. 28 - Loss: 1.0018401145935059. Acc: 0.5313807725906372\n",
      "Ep. 29 - Loss: 1.0029542446136475. Acc: 0.5313807725906372\n",
      "Ep. 30 - Loss: 0.9971885681152344. Acc: 0.5313807725906372\n",
      "Ep. 31 - Loss: 1.0029042959213257. Acc: 0.5313807725906372\n",
      "Ep. 32 - Loss: 1.010481595993042. Acc: 0.5313807725906372\n",
      "Ep. 33 - Loss: 1.0039557218551636. Acc: 0.5313807725906372\n",
      "Ep. 34 - Loss: 1.0081347227096558. Acc: 0.5313807725906372\n",
      "Ep. 35 - Loss: 1.0092542171478271. Acc: 0.5313807725906372\n",
      "Ep. 36 - Loss: 0.9993382096290588. Acc: 0.5313807725906372\n",
      "Ep. 37 - Loss: 1.007669448852539. Acc: 0.5313807725906372\n",
      "Ep. 38 - Loss: 1.0048695802688599. Acc: 0.5313807725906372\n",
      "Ep. 39 - Loss: 1.0052261352539062. Acc: 0.5313807725906372\n",
      "Ep. 40 - Loss: 1.0076484680175781. Acc: 0.5313807725906372\n",
      "Ep. 41 - Loss: 1.0028101205825806. Acc: 0.5313807725906372\n",
      "Ep. 42 - Loss: 0.9951084852218628. Acc: 0.5313807725906372\n",
      "Ep. 43 - Loss: 1.0027670860290527. Acc: 0.5313807725906372\n",
      "Ep. 44 - Loss: 1.0015323162078857. Acc: 0.5313807725906372\n",
      "Ep. 45 - Loss: 0.9999140501022339. Acc: 0.5313807725906372\n",
      "Ep. 46 - Loss: 0.9978005886077881. Acc: 0.5313807725906372\n",
      "Ep. 47 - Loss: 0.9988230466842651. Acc: 0.5313807725906372\n",
      "Ep. 48 - Loss: 0.9978986382484436. Acc: 0.5313807725906372\n",
      "Ep. 49 - Loss: 1.0024687051773071. Acc: 0.5313807725906372\n",
      "Ep. 50 - Loss: 1.0017507076263428. Acc: 0.5313807725906372\n",
      "Ep. 51 - Loss: 1.0057731866836548. Acc: 0.5313807725906372\n",
      "Ep. 52 - Loss: 1.0001283884048462. Acc: 0.5313807725906372\n",
      "Ep. 53 - Loss: 1.0091692209243774. Acc: 0.5313807725906372\n",
      "Ep. 54 - Loss: 1.0060991048812866. Acc: 0.5313807725906372\n",
      "Ep. 55 - Loss: 1.0014593601226807. Acc: 0.5313807725906372\n",
      "Ep. 56 - Loss: 0.999386727809906. Acc: 0.5313807725906372\n",
      "Ep. 57 - Loss: 0.9981743693351746. Acc: 0.5313807725906372\n",
      "Ep. 58 - Loss: 0.999022364616394. Acc: 0.5313807725906372\n",
      "Ep. 59 - Loss: 0.9987689256668091. Acc: 0.5313807725906372\n",
      "Ep. 60 - Loss: 1.0035642385482788. Acc: 0.5313807725906372\n",
      "Ep. 61 - Loss: 0.9962217211723328. Acc: 0.5313807725906372\n",
      "Ep. 62 - Loss: 0.9997725486755371. Acc: 0.5313807725906372\n",
      "Ep. 63 - Loss: 1.005456805229187. Acc: 0.5313807725906372\n",
      "Ep. 64 - Loss: 1.0003676414489746. Acc: 0.5313807725906372\n",
      "Ep. 65 - Loss: 0.998856782913208. Acc: 0.5313807725906372\n",
      "Ep. 66 - Loss: 1.0019471645355225. Acc: 0.5313807725906372\n",
      "Ep. 67 - Loss: 0.9951692223548889. Acc: 0.5313807725906372\n",
      "Ep. 68 - Loss: 1.0048826932907104. Acc: 0.5313807725906372\n",
      "Ep. 69 - Loss: 0.9972720146179199. Acc: 0.5313807725906372\n",
      "Ep. 70 - Loss: 0.9991921186447144. Acc: 0.5313807725906372\n",
      "Ep. 71 - Loss: 1.001573920249939. Acc: 0.5313807725906372\n",
      "Ep. 72 - Loss: 0.9992912411689758. Acc: 0.5313807725906372\n",
      "Ep. 73 - Loss: 0.9987286925315857. Acc: 0.5313807725906372\n",
      "Ep. 74 - Loss: 0.9998242855072021. Acc: 0.5313807725906372\n",
      "Ep. 75 - Loss: 1.0009543895721436. Acc: 0.5313807725906372\n",
      "Ep. 76 - Loss: 1.0023707151412964. Acc: 0.5313807725906372\n",
      "Ep. 77 - Loss: 1.0005992650985718. Acc: 0.5313807725906372\n",
      "Ep. 78 - Loss: 0.9977303147315979. Acc: 0.5313807725906372\n",
      "Ep. 79 - Loss: 1.006664752960205. Acc: 0.5313807725906372\n",
      "Ep. 80 - Loss: 1.0044819116592407. Acc: 0.5313807725906372\n",
      "Ep. 81 - Loss: 1.003186821937561. Acc: 0.5313807725906372\n",
      "Ep. 82 - Loss: 1.002073884010315. Acc: 0.5313807725906372\n",
      "Ep. 83 - Loss: 1.0073444843292236. Acc: 0.5313807725906372\n",
      "Ep. 84 - Loss: 1.0044361352920532. Acc: 0.5313807725906372\n",
      "Ep. 85 - Loss: 1.0001262426376343. Acc: 0.5313807725906372\n",
      "Ep. 86 - Loss: 1.002646565437317. Acc: 0.5313807725906372\n",
      "Ep. 87 - Loss: 1.0004593133926392. Acc: 0.5313807725906372\n",
      "Ep. 88 - Loss: 1.0009846687316895. Acc: 0.5313807725906372\n",
      "Ep. 89 - Loss: 0.9964763522148132. Acc: 0.5313807725906372\n",
      "Ep. 90 - Loss: 1.001379370689392. Acc: 0.5313807725906372\n",
      "Ep. 91 - Loss: 1.0035326480865479. Acc: 0.5313807725906372\n",
      "Ep. 92 - Loss: 1.0006828308105469. Acc: 0.5313807725906372\n",
      "Ep. 93 - Loss: 0.9995802640914917. Acc: 0.5313807725906372\n",
      "Ep. 94 - Loss: 1.0003379583358765. Acc: 0.5313807725906372\n",
      "Ep. 95 - Loss: 0.999268651008606. Acc: 0.5313807725906372\n",
      "Ep. 96 - Loss: 0.9992767572402954. Acc: 0.5313807725906372\n",
      "Ep. 97 - Loss: 0.9972900748252869. Acc: 0.5313807725906372\n",
      "Ep. 98 - Loss: 0.9912450909614563. Acc: 0.5313807725906372\n",
      "Ep. 99 - Loss: 1.0024091005325317. Acc: 0.5313807725906372\n",
      "Ep. 100 - Loss: 1.0010119676589966. Acc: 0.5313807725906372\n",
      "Ep. 101 - Loss: 0.998053789138794. Acc: 0.5313807725906372\n",
      "Ep. 102 - Loss: 0.9968962073326111. Acc: 0.5313807725906372\n",
      "Ep. 103 - Loss: 1.0024970769882202. Acc: 0.5313807725906372\n",
      "Ep. 104 - Loss: 1.0023947954177856. Acc: 0.5313807725906372\n",
      "Ep. 105 - Loss: 0.9978066086769104. Acc: 0.5313807725906372\n",
      "Ep. 106 - Loss: 0.9994046092033386. Acc: 0.5313807725906372\n",
      "Ep. 107 - Loss: 1.0019136667251587. Acc: 0.5313807725906372\n",
      "Ep. 108 - Loss: 0.9983541965484619. Acc: 0.5313807725906372\n",
      "Ep. 109 - Loss: 0.9989717602729797. Acc: 0.5313807725906372\n",
      "Ep. 110 - Loss: 1.0013618469238281. Acc: 0.5313807725906372\n",
      "Ep. 111 - Loss: 1.000449538230896. Acc: 0.5313807725906372\n",
      "Ep. 112 - Loss: 0.9980363249778748. Acc: 0.5313807725906372\n",
      "Ep. 113 - Loss: 1.0021482706069946. Acc: 0.5313807725906372\n",
      "Ep. 114 - Loss: 0.9981398582458496. Acc: 0.5313807725906372\n",
      "Ep. 115 - Loss: 0.9999246001243591. Acc: 0.5313807725906372\n",
      "Ep. 116 - Loss: 0.9941483736038208. Acc: 0.5313807725906372\n",
      "Ep. 117 - Loss: 1.0029793977737427. Acc: 0.5313807725906372\n",
      "Ep. 118 - Loss: 1.0027925968170166. Acc: 0.5313807725906372\n",
      "Ep. 119 - Loss: 0.9981878399848938. Acc: 0.5313807725906372\n",
      "Ep. 120 - Loss: 0.9970493316650391. Acc: 0.5313807725906372\n",
      "Ep. 121 - Loss: 0.9990967512130737. Acc: 0.5313807725906372\n",
      "Ep. 122 - Loss: 0.9978910088539124. Acc: 0.5313807725906372\n",
      "Ep. 123 - Loss: 0.9990295171737671. Acc: 0.5313807725906372\n",
      "Ep. 124 - Loss: 0.9986045360565186. Acc: 0.5313807725906372\n",
      "Ep. 125 - Loss: 0.9999843835830688. Acc: 0.5313807725906372\n",
      "Ep. 126 - Loss: 0.9986950755119324. Acc: 0.5313807725906372\n",
      "Ep. 127 - Loss: 0.9995076060295105. Acc: 0.5313807725906372\n",
      "Ep. 128 - Loss: 1.001893162727356. Acc: 0.5313807725906372\n",
      "Ep. 129 - Loss: 1.0002541542053223. Acc: 0.5313807725906372\n",
      "Ep. 130 - Loss: 1.0008739233016968. Acc: 0.5313807725906372\n",
      "Ep. 131 - Loss: 0.9983611702919006. Acc: 0.5313807725906372\n",
      "Ep. 132 - Loss: 0.9984433650970459. Acc: 0.5313807725906372\n",
      "Ep. 133 - Loss: 0.9997636079788208. Acc: 0.5313807725906372\n",
      "Ep. 134 - Loss: 0.997127890586853. Acc: 0.5313807725906372\n",
      "Ep. 135 - Loss: 0.9958669543266296. Acc: 0.5313807725906372\n",
      "Ep. 136 - Loss: 1.000656008720398. Acc: 0.5313807725906372\n",
      "Ep. 137 - Loss: 1.001004695892334. Acc: 0.5313807725906372\n",
      "Ep. 138 - Loss: 0.9985564351081848. Acc: 0.5313807725906372\n",
      "Ep. 139 - Loss: 0.9983026385307312. Acc: 0.5313807725906372\n",
      "Ep. 140 - Loss: 0.9975187182426453. Acc: 0.5313807725906372\n",
      "Ep. 141 - Loss: 1.0000901222229004. Acc: 0.5313807725906372\n",
      "Ep. 142 - Loss: 0.9987264275550842. Acc: 0.5313807725906372\n",
      "Ep. 143 - Loss: 0.9989029169082642. Acc: 0.5313807725906372\n",
      "Ep. 144 - Loss: 0.9979722499847412. Acc: 0.5313807725906372\n",
      "Ep. 145 - Loss: 1.0009667873382568. Acc: 0.5313807725906372\n",
      "Ep. 146 - Loss: 0.998898446559906. Acc: 0.5313807725906372\n",
      "Ep. 147 - Loss: 0.9993590116500854. Acc: 0.5313807725906372\n",
      "Ep. 148 - Loss: 0.9999414682388306. Acc: 0.5313807725906372\n",
      "Ep. 149 - Loss: 0.9979982972145081. Acc: 0.5313807725906372\n",
      "Ep. 150 - Loss: 0.9976454377174377. Acc: 0.5313807725906372\n",
      "Ep. 151 - Loss: 0.998078465461731. Acc: 0.5313807725906372\n",
      "Ep. 152 - Loss: 0.9967316389083862. Acc: 0.5313807725906372\n",
      "Ep. 153 - Loss: 0.9974130988121033. Acc: 0.5313807725906372\n",
      "Ep. 154 - Loss: 0.9962158799171448. Acc: 0.5313807725906372\n",
      "Ep. 155 - Loss: 0.9986457824707031. Acc: 0.5313807725906372\n",
      "Ep. 156 - Loss: 0.9960118532180786. Acc: 0.5313807725906372\n",
      "Ep. 157 - Loss: 0.9971190094947815. Acc: 0.5313807725906372\n",
      "Ep. 158 - Loss: 0.9937250018119812. Acc: 0.5313807725906372\n",
      "Ep. 159 - Loss: 0.9988141655921936. Acc: 0.5313807725906372\n",
      "Ep. 160 - Loss: 1.0003345012664795. Acc: 0.5313807725906372\n",
      "Ep. 161 - Loss: 0.9972074031829834. Acc: 0.5313807725906372\n",
      "Ep. 162 - Loss: 1.001029372215271. Acc: 0.5313807725906372\n",
      "Ep. 163 - Loss: 0.9986172318458557. Acc: 0.5313807725906372\n",
      "Ep. 164 - Loss: 0.9989181160926819. Acc: 0.5313807725906372\n",
      "Ep. 165 - Loss: 0.9947099685668945. Acc: 0.5313807725906372\n",
      "Ep. 166 - Loss: 1.003358006477356. Acc: 0.5313807725906372\n",
      "Ep. 167 - Loss: 1.0017976760864258. Acc: 0.5313807725906372\n",
      "Ep. 168 - Loss: 0.9986766576766968. Acc: 0.5313807725906372\n",
      "Ep. 169 - Loss: 0.9980292916297913. Acc: 0.5313807725906372\n",
      "Ep. 170 - Loss: 0.9983741044998169. Acc: 0.5313807725906372\n",
      "Ep. 171 - Loss: 0.998684287071228. Acc: 0.5313807725906372\n",
      "Ep. 172 - Loss: 0.9985483288764954. Acc: 0.5313807725906372\n",
      "Ep. 173 - Loss: 0.9976226091384888. Acc: 0.5313807725906372\n",
      "Ep. 174 - Loss: 0.9981824159622192. Acc: 0.5313807725906372\n",
      "Ep. 175 - Loss: 0.997768759727478. Acc: 0.5313807725906372\n",
      "Ep. 176 - Loss: 0.9991479516029358. Acc: 0.5313807725906372\n",
      "Ep. 177 - Loss: 0.9979512095451355. Acc: 0.5313807725906372\n",
      "Ep. 178 - Loss: 0.9992780089378357. Acc: 0.5313807725906372\n",
      "Ep. 179 - Loss: 0.9976542592048645. Acc: 0.5313807725906372\n",
      "Ep. 180 - Loss: 0.997382402420044. Acc: 0.5313807725906372\n",
      "Ep. 181 - Loss: 0.9976826906204224. Acc: 0.5313807725906372\n",
      "Ep. 182 - Loss: 0.9978077411651611. Acc: 0.5313807725906372\n",
      "Ep. 183 - Loss: 0.99680495262146. Acc: 0.5313807725906372\n",
      "Ep. 184 - Loss: 0.9980239272117615. Acc: 0.5313807725906372\n",
      "Ep. 185 - Loss: 0.9964412450790405. Acc: 0.5313807725906372\n",
      "Ep. 186 - Loss: 0.9973217844963074. Acc: 0.5313807725906372\n",
      "Ep. 187 - Loss: 0.9991361498832703. Acc: 0.5313807725906372\n",
      "Ep. 188 - Loss: 0.9996312260627747. Acc: 0.5313807725906372\n",
      "Ep. 189 - Loss: 0.9967007040977478. Acc: 0.5313807725906372\n",
      "Ep. 190 - Loss: 0.9984234571456909. Acc: 0.5313807725906372\n",
      "Ep. 191 - Loss: 0.9974789023399353. Acc: 0.5313807725906372\n",
      "Ep. 192 - Loss: 0.999147891998291. Acc: 0.5313807725906372\n",
      "Ep. 193 - Loss: 0.9976770877838135. Acc: 0.5313807725906372\n",
      "Ep. 194 - Loss: 0.9977832436561584. Acc: 0.5313807725906372\n",
      "Ep. 195 - Loss: 0.99826979637146. Acc: 0.5313807725906372\n",
      "Ep. 196 - Loss: 0.9972658753395081. Acc: 0.5313807725906372\n",
      "Ep. 197 - Loss: 0.9988230466842651. Acc: 0.5313807725906372\n",
      "Ep. 198 - Loss: 0.9986556172370911. Acc: 0.5313807725906372\n",
      "Ep. 199 - Loss: 0.9974064230918884. Acc: 0.5313807725906372\n",
      "Ep. 200 - Loss: 0.997776448726654. Acc: 0.5313807725906372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold:  40%|████      | 2/5 [19:07<28:33, 571.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Test loss: 0.9951092004776001. Test acc: 0.5333333611488342\n",
      "Ep. 1 - Loss: 1.036238193511963. Acc: 0.5062761306762695\n",
      "Ep. 2 - Loss: 1.0339317321777344. Acc: 0.5271966457366943\n",
      "Ep. 3 - Loss: 1.0158313512802124. Acc: 0.5313807725906372\n",
      "Ep. 4 - Loss: 1.0009472370147705. Acc: 0.5313807725906372\n",
      "Ep. 5 - Loss: 1.020891785621643. Acc: 0.5313807725906372\n",
      "Ep. 6 - Loss: 1.0078800916671753. Acc: 0.5313807725906372\n",
      "Ep. 7 - Loss: 1.003319501876831. Acc: 0.5313807725906372\n",
      "Ep. 8 - Loss: 1.0161159038543701. Acc: 0.5313807725906372\n",
      "Ep. 9 - Loss: 1.004173994064331. Acc: 0.5313807725906372\n",
      "Ep. 10 - Loss: 1.0010920763015747. Acc: 0.5313807725906372\n",
      "Ep. 11 - Loss: 1.0162560939788818. Acc: 0.5230125784873962\n",
      "Ep. 12 - Loss: 1.0019469261169434. Acc: 0.5313807725906372\n",
      "Ep. 13 - Loss: 1.0137765407562256. Acc: 0.5313807725906372\n",
      "Ep. 14 - Loss: 0.9912126064300537. Acc: 0.5271966457366943\n",
      "Ep. 15 - Loss: 1.0079878568649292. Acc: 0.5313807725906372\n",
      "Ep. 16 - Loss: 1.0062165260314941. Acc: 0.5313807725906372\n",
      "Ep. 17 - Loss: 1.0103117227554321. Acc: 0.5313807725906372\n",
      "Ep. 18 - Loss: 1.0148341655731201. Acc: 0.5313807725906372\n",
      "Ep. 19 - Loss: 1.0103868246078491. Acc: 0.5313807725906372\n",
      "Ep. 20 - Loss: 1.0040440559387207. Acc: 0.5313807725906372\n",
      "Ep. 21 - Loss: 1.0050902366638184. Acc: 0.5313807725906372\n",
      "Ep. 22 - Loss: 1.0039417743682861. Acc: 0.5313807725906372\n",
      "Ep. 23 - Loss: 1.0066583156585693. Acc: 0.5313807725906372\n",
      "Ep. 24 - Loss: 1.0082817077636719. Acc: 0.5313807725906372\n",
      "Ep. 25 - Loss: 1.0108373165130615. Acc: 0.5313807725906372\n",
      "Ep. 26 - Loss: 1.0067397356033325. Acc: 0.5313807725906372\n",
      "Ep. 27 - Loss: 1.012649416923523. Acc: 0.5313807725906372\n",
      "Ep. 28 - Loss: 1.0062917470932007. Acc: 0.5313807725906372\n",
      "Ep. 29 - Loss: 1.0003596544265747. Acc: 0.5313807725906372\n",
      "Ep. 30 - Loss: 1.0047358274459839. Acc: 0.5313807725906372\n",
      "Ep. 31 - Loss: 1.0011597871780396. Acc: 0.5313807725906372\n",
      "Ep. 32 - Loss: 1.007929801940918. Acc: 0.5313807725906372\n",
      "Ep. 33 - Loss: 1.002042293548584. Acc: 0.5313807725906372\n",
      "Ep. 34 - Loss: 0.9949980974197388. Acc: 0.5313807725906372\n",
      "Ep. 35 - Loss: 1.0019317865371704. Acc: 0.5313807725906372\n",
      "Ep. 36 - Loss: 0.9977613687515259. Acc: 0.5313807725906372\n",
      "Ep. 37 - Loss: 1.0007891654968262. Acc: 0.5313807725906372\n",
      "Ep. 38 - Loss: 0.9992382526397705. Acc: 0.5313807725906372\n",
      "Ep. 39 - Loss: 0.9995168447494507. Acc: 0.5313807725906372\n",
      "Ep. 40 - Loss: 1.0013930797576904. Acc: 0.5313807725906372\n",
      "Ep. 41 - Loss: 1.0103708505630493. Acc: 0.5313807725906372\n",
      "Ep. 42 - Loss: 1.0055159330368042. Acc: 0.5313807725906372\n",
      "Ep. 43 - Loss: 1.0032404661178589. Acc: 0.5313807725906372\n",
      "Ep. 44 - Loss: 1.0082998275756836. Acc: 0.5313807725906372\n",
      "Ep. 45 - Loss: 1.0069925785064697. Acc: 0.5355648398399353\n",
      "Ep. 46 - Loss: 1.0006078481674194. Acc: 0.5313807725906372\n",
      "Ep. 47 - Loss: 1.005111575126648. Acc: 0.5313807725906372\n",
      "Ep. 48 - Loss: 1.001501202583313. Acc: 0.5313807725906372\n",
      "Ep. 49 - Loss: 1.002954363822937. Acc: 0.5313807725906372\n",
      "Ep. 50 - Loss: 1.0017503499984741. Acc: 0.5313807725906372\n",
      "Ep. 51 - Loss: 1.0097507238388062. Acc: 0.5313807725906372\n",
      "Ep. 52 - Loss: 1.0041245222091675. Acc: 0.5313807725906372\n",
      "Ep. 53 - Loss: 1.006034255027771. Acc: 0.5313807725906372\n",
      "Ep. 54 - Loss: 1.0029621124267578. Acc: 0.5313807725906372\n",
      "Ep. 55 - Loss: 1.0075851678848267. Acc: 0.5313807725906372\n",
      "Ep. 56 - Loss: 1.00597083568573. Acc: 0.5313807725906372\n",
      "Ep. 57 - Loss: 0.9972524642944336. Acc: 0.5313807725906372\n",
      "Ep. 58 - Loss: 1.0066101551055908. Acc: 0.5313807725906372\n",
      "Ep. 59 - Loss: 1.0024443864822388. Acc: 0.5313807725906372\n",
      "Ep. 60 - Loss: 1.0012024641036987. Acc: 0.5313807725906372\n",
      "Ep. 61 - Loss: 0.9960596561431885. Acc: 0.5313807725906372\n",
      "Ep. 62 - Loss: 1.0016870498657227. Acc: 0.5313807725906372\n",
      "Ep. 63 - Loss: 1.0011944770812988. Acc: 0.5313807725906372\n",
      "Ep. 64 - Loss: 1.0035691261291504. Acc: 0.5313807725906372\n",
      "Ep. 65 - Loss: 1.002306580543518. Acc: 0.5313807725906372\n",
      "Ep. 66 - Loss: 1.0039337873458862. Acc: 0.5313807725906372\n",
      "Ep. 67 - Loss: 1.0021538734436035. Acc: 0.5313807725906372\n",
      "Ep. 68 - Loss: 1.0026921033859253. Acc: 0.5313807725906372\n",
      "Ep. 69 - Loss: 0.9994696378707886. Acc: 0.5313807725906372\n",
      "Ep. 70 - Loss: 1.0021593570709229. Acc: 0.5313807725906372\n",
      "Ep. 71 - Loss: 0.9967355132102966. Acc: 0.5313807725906372\n",
      "Ep. 72 - Loss: 1.0036426782608032. Acc: 0.5313807725906372\n",
      "Ep. 73 - Loss: 1.0033196210861206. Acc: 0.5313807725906372\n",
      "Ep. 74 - Loss: 1.0026861429214478. Acc: 0.5313807725906372\n",
      "Ep. 75 - Loss: 0.9980899095535278. Acc: 0.5313807725906372\n",
      "Ep. 76 - Loss: 1.0032248497009277. Acc: 0.5313807725906372\n",
      "Ep. 77 - Loss: 0.9991628527641296. Acc: 0.5313807725906372\n",
      "Ep. 78 - Loss: 1.0026168823242188. Acc: 0.5313807725906372\n",
      "Ep. 79 - Loss: 0.9966925382614136. Acc: 0.5313807725906372\n",
      "Ep. 80 - Loss: 1.0025724172592163. Acc: 0.5313807725906372\n",
      "Ep. 81 - Loss: 0.9954541921615601. Acc: 0.5313807725906372\n",
      "Ep. 82 - Loss: 0.9986153841018677. Acc: 0.5313807725906372\n",
      "Ep. 83 - Loss: 0.9942073822021484. Acc: 0.5313807725906372\n",
      "Ep. 84 - Loss: 1.0032670497894287. Acc: 0.5313807725906372\n",
      "Ep. 85 - Loss: 1.0014740228652954. Acc: 0.5313807725906372\n",
      "Ep. 86 - Loss: 1.0004335641860962. Acc: 0.5313807725906372\n",
      "Ep. 87 - Loss: 1.0025986433029175. Acc: 0.5313807725906372\n",
      "Ep. 88 - Loss: 1.002807378768921. Acc: 0.5313807725906372\n",
      "Ep. 89 - Loss: 0.9978634119033813. Acc: 0.5313807725906372\n",
      "Ep. 90 - Loss: 0.9995075464248657. Acc: 0.5313807725906372\n",
      "Ep. 91 - Loss: 0.9999238848686218. Acc: 0.5313807725906372\n",
      "Ep. 92 - Loss: 0.9983986616134644. Acc: 0.5313807725906372\n",
      "Ep. 93 - Loss: 1.003625750541687. Acc: 0.5313807725906372\n",
      "Ep. 94 - Loss: 1.0028921365737915. Acc: 0.5313807725906372\n",
      "Ep. 95 - Loss: 0.9981512427330017. Acc: 0.5313807725906372\n",
      "Ep. 96 - Loss: 0.9996872544288635. Acc: 0.5313807725906372\n",
      "Ep. 97 - Loss: 1.0019855499267578. Acc: 0.5313807725906372\n",
      "Ep. 98 - Loss: 0.9999185800552368. Acc: 0.5313807725906372\n",
      "Ep. 99 - Loss: 0.9976176619529724. Acc: 0.5313807725906372\n",
      "Ep. 100 - Loss: 0.9971242547035217. Acc: 0.5313807725906372\n",
      "Ep. 101 - Loss: 0.9959121942520142. Acc: 0.5313807725906372\n",
      "Ep. 102 - Loss: 1.001317024230957. Acc: 0.5313807725906372\n",
      "Ep. 103 - Loss: 0.9988639950752258. Acc: 0.5313807725906372\n",
      "Ep. 104 - Loss: 0.9988200664520264. Acc: 0.5313807725906372\n",
      "Ep. 105 - Loss: 0.9984387755393982. Acc: 0.5313807725906372\n",
      "Ep. 106 - Loss: 0.9991906881332397. Acc: 0.5313807725906372\n",
      "Ep. 107 - Loss: 0.9972814321517944. Acc: 0.5313807725906372\n",
      "Ep. 108 - Loss: 1.0012775659561157. Acc: 0.5313807725906372\n",
      "Ep. 109 - Loss: 0.9973163604736328. Acc: 0.5313807725906372\n",
      "Ep. 110 - Loss: 0.9939227104187012. Acc: 0.5313807725906372\n",
      "Ep. 111 - Loss: 0.9989429116249084. Acc: 0.5313807725906372\n",
      "Ep. 112 - Loss: 0.9946964979171753. Acc: 0.5313807725906372\n",
      "Ep. 113 - Loss: 1.001570224761963. Acc: 0.5313807725906372\n",
      "Ep. 114 - Loss: 0.9968920350074768. Acc: 0.5313807725906372\n",
      "Ep. 115 - Loss: 0.9983459711074829. Acc: 0.5313807725906372\n",
      "Ep. 116 - Loss: 1.0002975463867188. Acc: 0.5313807725906372\n",
      "Ep. 117 - Loss: 1.000070333480835. Acc: 0.5313807725906372\n",
      "Ep. 118 - Loss: 0.9970735311508179. Acc: 0.5313807725906372\n",
      "Ep. 119 - Loss: 1.0002423524856567. Acc: 0.5313807725906372\n",
      "Ep. 120 - Loss: 0.9999392628669739. Acc: 0.5313807725906372\n",
      "Ep. 121 - Loss: 1.0016851425170898. Acc: 0.5313807725906372\n",
      "Ep. 122 - Loss: 1.00169038772583. Acc: 0.5313807725906372\n",
      "Ep. 123 - Loss: 0.9999310970306396. Acc: 0.5313807725906372\n",
      "Ep. 124 - Loss: 0.996467113494873. Acc: 0.5313807725906372\n",
      "Ep. 125 - Loss: 0.9994632005691528. Acc: 0.5313807725906372\n",
      "Ep. 126 - Loss: 0.998520016670227. Acc: 0.5313807725906372\n",
      "Ep. 127 - Loss: 0.9988843202590942. Acc: 0.5313807725906372\n",
      "Ep. 128 - Loss: 1.0000507831573486. Acc: 0.5313807725906372\n",
      "Ep. 129 - Loss: 0.9979333877563477. Acc: 0.5313807725906372\n",
      "Ep. 130 - Loss: 0.9989630579948425. Acc: 0.5313807725906372\n",
      "Ep. 131 - Loss: 0.9991611838340759. Acc: 0.5313807725906372\n",
      "Ep. 132 - Loss: 0.9974300861358643. Acc: 0.5313807725906372\n",
      "Ep. 133 - Loss: 1.000178337097168. Acc: 0.5313807725906372\n",
      "Ep. 134 - Loss: 0.9975528120994568. Acc: 0.5313807725906372\n",
      "Ep. 135 - Loss: 0.9982461333274841. Acc: 0.5313807725906372\n",
      "Ep. 136 - Loss: 0.9971116781234741. Acc: 0.5313807725906372\n",
      "Ep. 137 - Loss: 0.9992710947990417. Acc: 0.5313807725906372\n",
      "Ep. 138 - Loss: 0.9987250566482544. Acc: 0.5313807725906372\n",
      "Ep. 139 - Loss: 0.9996519684791565. Acc: 0.5313807725906372\n",
      "Ep. 140 - Loss: 1.0014795064926147. Acc: 0.5313807725906372\n",
      "Ep. 141 - Loss: 0.9970000982284546. Acc: 0.5313807725906372\n",
      "Ep. 142 - Loss: 0.9969214200973511. Acc: 0.5313807725906372\n",
      "Ep. 143 - Loss: 0.9953740239143372. Acc: 0.5313807725906372\n",
      "Ep. 144 - Loss: 0.9996447563171387. Acc: 0.5313807725906372\n",
      "Ep. 145 - Loss: 0.9964281320571899. Acc: 0.5313807725906372\n",
      "Ep. 146 - Loss: 0.9972445964813232. Acc: 0.5313807725906372\n",
      "Ep. 147 - Loss: 0.9966527819633484. Acc: 0.5313807725906372\n",
      "Ep. 148 - Loss: 0.9971029758453369. Acc: 0.5313807725906372\n",
      "Ep. 149 - Loss: 0.9988318681716919. Acc: 0.5313807725906372\n",
      "Ep. 150 - Loss: 0.9995443820953369. Acc: 0.5313807725906372\n",
      "Ep. 151 - Loss: 0.9979857206344604. Acc: 0.5313807725906372\n",
      "Ep. 152 - Loss: 0.9966924786567688. Acc: 0.5313807725906372\n",
      "Ep. 153 - Loss: 0.9982571601867676. Acc: 0.5313807725906372\n",
      "Ep. 154 - Loss: 0.9981030225753784. Acc: 0.5313807725906372\n",
      "Ep. 155 - Loss: 0.9972994327545166. Acc: 0.5313807725906372\n",
      "Ep. 156 - Loss: 1.000260829925537. Acc: 0.5313807725906372\n",
      "Ep. 157 - Loss: 0.9976707100868225. Acc: 0.5313807725906372\n",
      "Ep. 158 - Loss: 0.9981828927993774. Acc: 0.5313807725906372\n",
      "Ep. 159 - Loss: 0.9983109831809998. Acc: 0.5313807725906372\n",
      "Ep. 160 - Loss: 0.9994390606880188. Acc: 0.5313807725906372\n",
      "Ep. 161 - Loss: 0.9989609718322754. Acc: 0.5313807725906372\n",
      "Ep. 162 - Loss: 0.9985418319702148. Acc: 0.5313807725906372\n",
      "Ep. 163 - Loss: 0.9981417059898376. Acc: 0.5313807725906372\n",
      "Ep. 164 - Loss: 0.9979770183563232. Acc: 0.5313807725906372\n",
      "Ep. 165 - Loss: 0.9981711506843567. Acc: 0.5313807725906372\n",
      "Ep. 166 - Loss: 0.9974706768989563. Acc: 0.5313807725906372\n",
      "Ep. 167 - Loss: 0.9981412291526794. Acc: 0.5313807725906372\n",
      "Ep. 168 - Loss: 0.998562753200531. Acc: 0.5313807725906372\n",
      "Ep. 169 - Loss: 0.9987902641296387. Acc: 0.5313807725906372\n",
      "Ep. 170 - Loss: 0.9967604279518127. Acc: 0.5313807725906372\n",
      "Ep. 171 - Loss: 0.9968568682670593. Acc: 0.5313807725906372\n",
      "Ep. 172 - Loss: 0.9958685040473938. Acc: 0.5313807725906372\n",
      "Ep. 173 - Loss: 1.0007203817367554. Acc: 0.5313807725906372\n",
      "Ep. 174 - Loss: 0.9977701306343079. Acc: 0.5313807725906372\n",
      "Ep. 175 - Loss: 0.9981297850608826. Acc: 0.5313807725906372\n",
      "Ep. 176 - Loss: 0.9983298182487488. Acc: 0.5313807725906372\n",
      "Ep. 177 - Loss: 0.9985654950141907. Acc: 0.5313807725906372\n",
      "Ep. 178 - Loss: 0.9978207349777222. Acc: 0.5313807725906372\n",
      "Ep. 179 - Loss: 0.997657299041748. Acc: 0.5313807725906372\n",
      "Ep. 180 - Loss: 0.9970881342887878. Acc: 0.5313807725906372\n",
      "Ep. 181 - Loss: 0.9976126551628113. Acc: 0.5313807725906372\n",
      "Ep. 182 - Loss: 0.9977315664291382. Acc: 0.5313807725906372\n",
      "Ep. 183 - Loss: 0.9974475502967834. Acc: 0.5313807725906372\n",
      "Ep. 184 - Loss: 0.9973195195198059. Acc: 0.5313807725906372\n",
      "Ep. 185 - Loss: 0.9976717233657837. Acc: 0.5313807725906372\n",
      "Ep. 186 - Loss: 0.9975124597549438. Acc: 0.5313807725906372\n",
      "Ep. 187 - Loss: 0.9972848296165466. Acc: 0.5313807725906372\n",
      "Ep. 188 - Loss: 0.9971802234649658. Acc: 0.5313807725906372\n",
      "Ep. 189 - Loss: 0.9973523616790771. Acc: 0.5313807725906372\n",
      "Ep. 190 - Loss: 0.9971908926963806. Acc: 0.5313807725906372\n",
      "Ep. 191 - Loss: 0.9971069693565369. Acc: 0.5313807725906372\n",
      "Ep. 192 - Loss: 0.9969976544380188. Acc: 0.5313807725906372\n",
      "Ep. 193 - Loss: 0.9969874620437622. Acc: 0.5313807725906372\n",
      "Ep. 194 - Loss: 0.9987765550613403. Acc: 0.5313807725906372\n",
      "Ep. 195 - Loss: 0.9966747760772705. Acc: 0.5313807725906372\n",
      "Ep. 196 - Loss: 0.9970788955688477. Acc: 0.5313807725906372\n",
      "Ep. 197 - Loss: 0.997376561164856. Acc: 0.5313807725906372\n",
      "Ep. 198 - Loss: 0.9973936080932617. Acc: 0.5313807725906372\n",
      "Ep. 199 - Loss: 0.9970454573631287. Acc: 0.5313807725906372\n",
      "Ep. 200 - Loss: 0.9979639053344727. Acc: 0.5313807725906372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold:  60%|██████    | 3/5 [28:41<19:05, 572.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Test loss: 0.9951087236404419. Test acc: 0.5333333611488342\n",
      "Ep. 1 - Loss: 1.0318124294281006. Acc: 0.5313807725906372\n",
      "Ep. 2 - Loss: 1.0346875190734863. Acc: 0.5313807725906372\n",
      "Ep. 3 - Loss: 1.0032399892807007. Acc: 0.5271966457366943\n",
      "Ep. 4 - Loss: 1.0123989582061768. Acc: 0.5271966457366943\n",
      "Ep. 5 - Loss: 1.0176922082901. Acc: 0.5397489666938782\n",
      "Ep. 6 - Loss: 0.9976193308830261. Acc: 0.5230125784873962\n",
      "Ep. 7 - Loss: 1.014035940170288. Acc: 0.5313807725906372\n",
      "Ep. 8 - Loss: 1.0176730155944824. Acc: 0.5313807725906372\n",
      "Ep. 9 - Loss: 1.0086634159088135. Acc: 0.5313807725906372\n",
      "Ep. 10 - Loss: 1.0048010349273682. Acc: 0.5313807725906372\n",
      "Ep. 11 - Loss: 1.0049560070037842. Acc: 0.5313807725906372\n",
      "Ep. 12 - Loss: 1.0146633386611938. Acc: 0.5313807725906372\n",
      "Ep. 13 - Loss: 1.0062655210494995. Acc: 0.5313807725906372\n",
      "Ep. 14 - Loss: 1.010806918144226. Acc: 0.5313807725906372\n",
      "Ep. 15 - Loss: 1.0128628015518188. Acc: 0.5313807725906372\n",
      "Ep. 16 - Loss: 1.0084784030914307. Acc: 0.5313807725906372\n",
      "Ep. 17 - Loss: 1.0064767599105835. Acc: 0.5313807725906372\n",
      "Ep. 18 - Loss: 1.0125924348831177. Acc: 0.5313807725906372\n",
      "Ep. 19 - Loss: 1.0115407705307007. Acc: 0.5313807725906372\n",
      "Ep. 20 - Loss: 1.006530523300171. Acc: 0.5313807725906372\n",
      "Ep. 21 - Loss: 1.0106281042099. Acc: 0.5313807725906372\n",
      "Ep. 22 - Loss: 0.997775137424469. Acc: 0.5313807725906372\n",
      "Ep. 23 - Loss: 0.9968221783638. Acc: 0.5313807725906372\n",
      "Ep. 24 - Loss: 1.0067905187606812. Acc: 0.5313807725906372\n",
      "Ep. 25 - Loss: 1.005603313446045. Acc: 0.5313807725906372\n",
      "Ep. 26 - Loss: 1.0070825815200806. Acc: 0.5313807725906372\n",
      "Ep. 27 - Loss: 1.013991117477417. Acc: 0.5313807725906372\n",
      "Ep. 28 - Loss: 1.008997917175293. Acc: 0.5313807725906372\n",
      "Ep. 29 - Loss: 1.0075825452804565. Acc: 0.5313807725906372\n",
      "Ep. 30 - Loss: 1.0042481422424316. Acc: 0.5313807725906372\n",
      "Ep. 31 - Loss: 1.0056121349334717. Acc: 0.5313807725906372\n",
      "Ep. 32 - Loss: 1.0131374597549438. Acc: 0.5313807725906372\n",
      "Ep. 33 - Loss: 1.0048582553863525. Acc: 0.5313807725906372\n",
      "Ep. 34 - Loss: 1.0102784633636475. Acc: 0.5313807725906372\n",
      "Ep. 35 - Loss: 0.9938907027244568. Acc: 0.5313807725906372\n",
      "Ep. 36 - Loss: 1.011359453201294. Acc: 0.5313807725906372\n",
      "Ep. 37 - Loss: 1.007102370262146. Acc: 0.5313807725906372\n",
      "Ep. 38 - Loss: 0.999488890171051. Acc: 0.5313807725906372\n",
      "Ep. 39 - Loss: 1.0066763162612915. Acc: 0.5313807725906372\n",
      "Ep. 40 - Loss: 0.9986456632614136. Acc: 0.5313807725906372\n",
      "Ep. 41 - Loss: 1.0034966468811035. Acc: 0.5313807725906372\n",
      "Ep. 42 - Loss: 1.0023553371429443. Acc: 0.5313807725906372\n",
      "Ep. 43 - Loss: 1.0034157037734985. Acc: 0.5313807725906372\n",
      "Ep. 44 - Loss: 1.0071918964385986. Acc: 0.5313807725906372\n",
      "Ep. 45 - Loss: 1.004588007926941. Acc: 0.5313807725906372\n",
      "Ep. 46 - Loss: 1.001639485359192. Acc: 0.5313807725906372\n",
      "Ep. 47 - Loss: 1.0016279220581055. Acc: 0.5313807725906372\n",
      "Ep. 48 - Loss: 1.0051038265228271. Acc: 0.5313807725906372\n",
      "Ep. 49 - Loss: 1.0026359558105469. Acc: 0.5313807725906372\n",
      "Ep. 50 - Loss: 0.9993536472320557. Acc: 0.5313807725906372\n",
      "Ep. 51 - Loss: 1.0030934810638428. Acc: 0.5313807725906372\n",
      "Ep. 52 - Loss: 0.9978300333023071. Acc: 0.5313807725906372\n",
      "Ep. 53 - Loss: 1.0031402111053467. Acc: 0.5313807725906372\n",
      "Ep. 54 - Loss: 1.006130576133728. Acc: 0.5313807725906372\n",
      "Ep. 55 - Loss: 1.002113699913025. Acc: 0.5313807725906372\n",
      "Ep. 56 - Loss: 1.0024802684783936. Acc: 0.5313807725906372\n",
      "Ep. 57 - Loss: 0.9996219873428345. Acc: 0.5313807725906372\n",
      "Ep. 58 - Loss: 1.0026017427444458. Acc: 0.5313807725906372\n",
      "Ep. 59 - Loss: 1.0041404962539673. Acc: 0.5313807725906372\n",
      "Ep. 60 - Loss: 1.0033724308013916. Acc: 0.5313807725906372\n",
      "Ep. 61 - Loss: 1.0014419555664062. Acc: 0.5313807725906372\n",
      "Ep. 62 - Loss: 1.0022634267807007. Acc: 0.5313807725906372\n",
      "Ep. 63 - Loss: 1.001133680343628. Acc: 0.5313807725906372\n",
      "Ep. 64 - Loss: 1.0059549808502197. Acc: 0.5313807725906372\n",
      "Ep. 65 - Loss: 1.001095175743103. Acc: 0.5313807725906372\n",
      "Ep. 66 - Loss: 0.9969949722290039. Acc: 0.5313807725906372\n",
      "Ep. 67 - Loss: 1.0041074752807617. Acc: 0.5313807725906372\n",
      "Ep. 68 - Loss: 0.998383104801178. Acc: 0.5313807725906372\n",
      "Ep. 69 - Loss: 0.994508683681488. Acc: 0.5313807725906372\n",
      "Ep. 70 - Loss: 0.9995501041412354. Acc: 0.5313807725906372\n",
      "Ep. 71 - Loss: 0.999935507774353. Acc: 0.5313807725906372\n",
      "Ep. 72 - Loss: 1.0018662214279175. Acc: 0.5313807725906372\n",
      "Ep. 73 - Loss: 0.9957548379898071. Acc: 0.5313807725906372\n",
      "Ep. 74 - Loss: 1.0019928216934204. Acc: 0.5313807725906372\n",
      "Ep. 75 - Loss: 0.9993460178375244. Acc: 0.5313807725906372\n",
      "Ep. 76 - Loss: 0.9985599517822266. Acc: 0.5313807725906372\n",
      "Ep. 77 - Loss: 1.0000836849212646. Acc: 0.5313807725906372\n",
      "Ep. 78 - Loss: 1.000462293624878. Acc: 0.5313807725906372\n",
      "Ep. 79 - Loss: 1.0019656419754028. Acc: 0.5313807725906372\n",
      "Ep. 80 - Loss: 0.9969565868377686. Acc: 0.5313807725906372\n",
      "Ep. 81 - Loss: 1.0016043186187744. Acc: 0.5313807725906372\n",
      "Ep. 82 - Loss: 1.0025466680526733. Acc: 0.5313807725906372\n",
      "Ep. 83 - Loss: 1.002849817276001. Acc: 0.5313807725906372\n",
      "Ep. 84 - Loss: 0.9981459379196167. Acc: 0.5313807725906372\n",
      "Ep. 85 - Loss: 0.9979574084281921. Acc: 0.5313807725906372\n",
      "Ep. 86 - Loss: 1.002561330795288. Acc: 0.5313807725906372\n",
      "Ep. 87 - Loss: 1.00143563747406. Acc: 0.5313807725906372\n",
      "Ep. 88 - Loss: 0.9971420764923096. Acc: 0.5313807725906372\n",
      "Ep. 89 - Loss: 1.0012869834899902. Acc: 0.5313807725906372\n",
      "Ep. 90 - Loss: 1.0003596544265747. Acc: 0.5313807725906372\n",
      "Ep. 91 - Loss: 0.9973108768463135. Acc: 0.5313807725906372\n",
      "Ep. 92 - Loss: 1.0031508207321167. Acc: 0.5313807725906372\n",
      "Ep. 93 - Loss: 1.0011564493179321. Acc: 0.5313807725906372\n",
      "Ep. 94 - Loss: 0.9992402791976929. Acc: 0.5313807725906372\n",
      "Ep. 95 - Loss: 1.0002659559249878. Acc: 0.5313807725906372\n",
      "Ep. 96 - Loss: 1.0021392107009888. Acc: 0.5313807725906372\n",
      "Ep. 97 - Loss: 0.9986479878425598. Acc: 0.5313807725906372\n",
      "Ep. 98 - Loss: 1.005624771118164. Acc: 0.5313807725906372\n",
      "Ep. 99 - Loss: 1.002886414527893. Acc: 0.5313807725906372\n",
      "Ep. 100 - Loss: 1.0017553567886353. Acc: 0.5313807725906372\n",
      "Ep. 101 - Loss: 1.0002975463867188. Acc: 0.5313807725906372\n",
      "Ep. 102 - Loss: 1.0021923780441284. Acc: 0.5313807725906372\n",
      "Ep. 103 - Loss: 0.9991989731788635. Acc: 0.5313807725906372\n",
      "Ep. 104 - Loss: 1.0006639957427979. Acc: 0.5313807725906372\n",
      "Ep. 105 - Loss: 1.0000603199005127. Acc: 0.5313807725906372\n",
      "Ep. 106 - Loss: 1.0003576278686523. Acc: 0.5313807725906372\n",
      "Ep. 107 - Loss: 1.0009902715682983. Acc: 0.5313807725906372\n",
      "Ep. 108 - Loss: 0.99644535779953. Acc: 0.5313807725906372\n",
      "Ep. 109 - Loss: 0.9990875124931335. Acc: 0.5313807725906372\n",
      "Ep. 110 - Loss: 0.9994445443153381. Acc: 0.5313807725906372\n",
      "Ep. 111 - Loss: 0.9983002543449402. Acc: 0.5313807725906372\n",
      "Ep. 112 - Loss: 0.9974294304847717. Acc: 0.5313807725906372\n",
      "Ep. 113 - Loss: 1.002346396446228. Acc: 0.5313807725906372\n",
      "Ep. 114 - Loss: 0.9977045059204102. Acc: 0.5313807725906372\n",
      "Ep. 115 - Loss: 0.9975829720497131. Acc: 0.5313807725906372\n",
      "Ep. 116 - Loss: 1.0010032653808594. Acc: 0.5313807725906372\n",
      "Ep. 117 - Loss: 1.0010886192321777. Acc: 0.5313807725906372\n",
      "Ep. 118 - Loss: 0.9965235590934753. Acc: 0.5313807725906372\n",
      "Ep. 119 - Loss: 1.0006119012832642. Acc: 0.5313807725906372\n",
      "Ep. 120 - Loss: 1.0015922784805298. Acc: 0.5271966457366943\n",
      "Ep. 121 - Loss: 0.9963855147361755. Acc: 0.5313807725906372\n",
      "Ep. 122 - Loss: 0.9998725652694702. Acc: 0.5313807725906372\n",
      "Ep. 123 - Loss: 1.0000370740890503. Acc: 0.5313807725906372\n",
      "Ep. 124 - Loss: 1.0017722845077515. Acc: 0.5313807725906372\n",
      "Ep. 125 - Loss: 1.0012061595916748. Acc: 0.5313807725906372\n",
      "Ep. 126 - Loss: 1.0004960298538208. Acc: 0.5313807725906372\n",
      "Ep. 127 - Loss: 1.000550627708435. Acc: 0.5313807725906372\n",
      "Ep. 128 - Loss: 0.9999691843986511. Acc: 0.5313807725906372\n",
      "Ep. 129 - Loss: 0.9970963001251221. Acc: 0.5313807725906372\n",
      "Ep. 130 - Loss: 0.9987198114395142. Acc: 0.5313807725906372\n",
      "Ep. 131 - Loss: 0.9977486729621887. Acc: 0.5313807725906372\n",
      "Ep. 132 - Loss: 0.9974418878555298. Acc: 0.5313807725906372\n",
      "Ep. 133 - Loss: 0.9988824725151062. Acc: 0.5313807725906372\n",
      "Ep. 134 - Loss: 0.9966070652008057. Acc: 0.5313807725906372\n",
      "Ep. 135 - Loss: 0.9973708391189575. Acc: 0.5313807725906372\n",
      "Ep. 136 - Loss: 0.9999760389328003. Acc: 0.5313807725906372\n",
      "Ep. 137 - Loss: 0.9992008209228516. Acc: 0.5313807725906372\n",
      "Ep. 138 - Loss: 0.9988803863525391. Acc: 0.5313807725906372\n",
      "Ep. 139 - Loss: 0.997024655342102. Acc: 0.5313807725906372\n",
      "Ep. 140 - Loss: 0.9985359311103821. Acc: 0.5313807725906372\n",
      "Ep. 141 - Loss: 1.0000660419464111. Acc: 0.5313807725906372\n",
      "Ep. 142 - Loss: 0.9983853697776794. Acc: 0.5313807725906372\n",
      "Ep. 143 - Loss: 0.997797429561615. Acc: 0.5313807725906372\n",
      "Ep. 144 - Loss: 0.9977899193763733. Acc: 0.5313807725906372\n",
      "Ep. 145 - Loss: 0.998294472694397. Acc: 0.5313807725906372\n",
      "Ep. 146 - Loss: 0.9987604022026062. Acc: 0.5313807725906372\n",
      "Ep. 147 - Loss: 1.0003883838653564. Acc: 0.5313807725906372\n",
      "Ep. 148 - Loss: 0.9983115792274475. Acc: 0.5313807725906372\n",
      "Ep. 149 - Loss: 0.9976689219474792. Acc: 0.5313807725906372\n",
      "Ep. 150 - Loss: 0.9987143874168396. Acc: 0.5313807725906372\n",
      "Ep. 151 - Loss: 0.9969269633293152. Acc: 0.5313807725906372\n",
      "Ep. 152 - Loss: 0.9978452324867249. Acc: 0.5313807725906372\n",
      "Ep. 153 - Loss: 0.9983898997306824. Acc: 0.5313807725906372\n",
      "Ep. 154 - Loss: 0.9976426959037781. Acc: 0.5313807725906372\n",
      "Ep. 155 - Loss: 0.9982560276985168. Acc: 0.5313807725906372\n",
      "Ep. 156 - Loss: 0.9969810843467712. Acc: 0.5313807725906372\n",
      "Ep. 157 - Loss: 0.998209536075592. Acc: 0.5313807725906372\n",
      "Ep. 158 - Loss: 0.9972286224365234. Acc: 0.5313807725906372\n",
      "Ep. 159 - Loss: 0.9975272417068481. Acc: 0.5313807725906372\n",
      "Ep. 160 - Loss: 0.9985043406486511. Acc: 0.5313807725906372\n",
      "Ep. 161 - Loss: 0.9982467889785767. Acc: 0.5313807725906372\n",
      "Ep. 162 - Loss: 0.9976841807365417. Acc: 0.5313807725906372\n",
      "Ep. 163 - Loss: 0.9970201253890991. Acc: 0.5313807725906372\n",
      "Ep. 164 - Loss: 0.9977089762687683. Acc: 0.5313807725906372\n",
      "Ep. 165 - Loss: 0.9973969459533691. Acc: 0.5313807725906372\n",
      "Ep. 166 - Loss: 0.9973610639572144. Acc: 0.5313807725906372\n",
      "Ep. 167 - Loss: 0.9969007968902588. Acc: 0.5313807725906372\n",
      "Ep. 168 - Loss: 0.9981153011322021. Acc: 0.5313807725906372\n",
      "Ep. 169 - Loss: 0.9973536729812622. Acc: 0.5313807725906372\n",
      "Ep. 170 - Loss: 0.9973289370536804. Acc: 0.5313807725906372\n",
      "Ep. 171 - Loss: 0.9971561431884766. Acc: 0.5313807725906372\n",
      "Ep. 172 - Loss: 0.9972013831138611. Acc: 0.5313807725906372\n",
      "Ep. 173 - Loss: 0.996925413608551. Acc: 0.5313807725906372\n",
      "Ep. 174 - Loss: 0.9984095096588135. Acc: 0.5313807725906372\n",
      "Ep. 175 - Loss: 0.998475968837738. Acc: 0.5313807725906372\n",
      "Ep. 176 - Loss: 0.9969020485877991. Acc: 0.5313807725906372\n",
      "Ep. 177 - Loss: 0.998884379863739. Acc: 0.5313807725906372\n",
      "Ep. 178 - Loss: 0.9976192712783813. Acc: 0.5313807725906372\n",
      "Ep. 179 - Loss: 0.9975589513778687. Acc: 0.5313807725906372\n",
      "Ep. 180 - Loss: 0.9957731366157532. Acc: 0.5313807725906372\n",
      "Ep. 181 - Loss: 0.9982197880744934. Acc: 0.5313807725906372\n",
      "Ep. 182 - Loss: 0.9969920516014099. Acc: 0.5313807725906372\n",
      "Ep. 183 - Loss: 0.9970517754554749. Acc: 0.5313807725906372\n",
      "Ep. 184 - Loss: 0.9985108971595764. Acc: 0.5313807725906372\n",
      "Ep. 185 - Loss: 0.9959259033203125. Acc: 0.5313807725906372\n",
      "Ep. 186 - Loss: 0.9986642599105835. Acc: 0.5313807725906372\n",
      "Ep. 187 - Loss: 0.9966892004013062. Acc: 0.5313807725906372\n",
      "Ep. 188 - Loss: 0.9975709915161133. Acc: 0.5313807725906372\n",
      "Ep. 189 - Loss: 0.9961100220680237. Acc: 0.5313807725906372\n",
      "Ep. 190 - Loss: 0.9974339604377747. Acc: 0.5313807725906372\n",
      "Ep. 191 - Loss: 0.9979742169380188. Acc: 0.5313807725906372\n",
      "Ep. 192 - Loss: 0.9972835779190063. Acc: 0.5313807725906372\n",
      "Ep. 193 - Loss: 0.9980360269546509. Acc: 0.5313807725906372\n",
      "Ep. 194 - Loss: 0.9979148507118225. Acc: 0.5313807725906372\n",
      "Ep. 195 - Loss: 0.9972836971282959. Acc: 0.5313807725906372\n",
      "Ep. 196 - Loss: 0.9975278377532959. Acc: 0.5313807725906372\n",
      "Ep. 197 - Loss: 0.9972259998321533. Acc: 0.5313807725906372\n",
      "Ep. 198 - Loss: 0.9973600506782532. Acc: 0.5313807725906372\n",
      "Ep. 199 - Loss: 0.9972974061965942. Acc: 0.5313807725906372\n",
      "Ep. 200 - Loss: 0.9975969195365906. Acc: 0.5313807725906372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold:  80%|████████  | 4/5 [38:23<09:36, 576.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Test loss: 0.9951061010360718. Test acc: 0.5333333611488342\n",
      "Ep. 1 - Loss: 1.0479801893234253. Acc: 0.5041666626930237\n",
      "Ep. 2 - Loss: 1.0320168733596802. Acc: 0.5333333611488342\n",
      "Ep. 3 - Loss: 1.0134187936782837. Acc: 0.5333333611488342\n",
      "Ep. 4 - Loss: 1.0159051418304443. Acc: 0.5333333611488342\n",
      "Ep. 5 - Loss: 1.0126702785491943. Acc: 0.5333333611488342\n",
      "Ep. 6 - Loss: 1.0251668691635132. Acc: 0.5333333611488342\n",
      "Ep. 7 - Loss: 1.0114772319793701. Acc: 0.5333333611488342\n",
      "Ep. 8 - Loss: 1.0178523063659668. Acc: 0.5333333611488342\n",
      "Ep. 9 - Loss: 1.0004714727401733. Acc: 0.5333333611488342\n",
      "Ep. 10 - Loss: 1.010414719581604. Acc: 0.5333333611488342\n",
      "Ep. 11 - Loss: 1.010692834854126. Acc: 0.5333333611488342\n",
      "Ep. 12 - Loss: 1.0047698020935059. Acc: 0.5333333611488342\n",
      "Ep. 13 - Loss: 1.0048631429672241. Acc: 0.5333333611488342\n",
      "Ep. 14 - Loss: 1.0112051963806152. Acc: 0.5333333611488342\n",
      "Ep. 15 - Loss: 1.0020415782928467. Acc: 0.5333333611488342\n",
      "Ep. 16 - Loss: 1.0056232213974. Acc: 0.5333333611488342\n",
      "Ep. 17 - Loss: 1.005285620689392. Acc: 0.5333333611488342\n",
      "Ep. 18 - Loss: 1.0028963088989258. Acc: 0.5333333611488342\n",
      "Ep. 19 - Loss: 0.9972732663154602. Acc: 0.5333333611488342\n",
      "Ep. 20 - Loss: 1.0035369396209717. Acc: 0.5333333611488342\n",
      "Ep. 21 - Loss: 1.0056883096694946. Acc: 0.5333333611488342\n",
      "Ep. 22 - Loss: 1.0017999410629272. Acc: 0.5333333611488342\n",
      "Ep. 23 - Loss: 1.005836844444275. Acc: 0.5333333611488342\n",
      "Ep. 24 - Loss: 1.0068771839141846. Acc: 0.5333333611488342\n",
      "Ep. 25 - Loss: 1.0047515630722046. Acc: 0.5333333611488342\n",
      "Ep. 26 - Loss: 1.00142502784729. Acc: 0.5333333611488342\n",
      "Ep. 27 - Loss: 1.0019198656082153. Acc: 0.5333333611488342\n",
      "Ep. 28 - Loss: 1.000881314277649. Acc: 0.5333333611488342\n",
      "Ep. 29 - Loss: 1.0017037391662598. Acc: 0.5333333611488342\n",
      "Ep. 30 - Loss: 0.9975528717041016. Acc: 0.5333333611488342\n",
      "Ep. 31 - Loss: 0.9997573494911194. Acc: 0.5333333611488342\n",
      "Ep. 32 - Loss: 1.0049564838409424. Acc: 0.5333333611488342\n",
      "Ep. 33 - Loss: 1.0061867237091064. Acc: 0.5333333611488342\n",
      "Ep. 34 - Loss: 1.00410795211792. Acc: 0.5333333611488342\n",
      "Ep. 35 - Loss: 0.9978578686714172. Acc: 0.5333333611488342\n",
      "Ep. 36 - Loss: 0.9984816312789917. Acc: 0.5333333611488342\n",
      "Ep. 37 - Loss: 0.9924857020378113. Acc: 0.5333333611488342\n",
      "Ep. 38 - Loss: 0.9932785630226135. Acc: 0.5333333611488342\n",
      "Ep. 39 - Loss: 1.0046844482421875. Acc: 0.5333333611488342\n",
      "Ep. 40 - Loss: 0.9941283464431763. Acc: 0.5333333611488342\n",
      "Ep. 41 - Loss: 1.0042054653167725. Acc: 0.5333333611488342\n",
      "Ep. 42 - Loss: 0.9997723698616028. Acc: 0.5333333611488342\n",
      "Ep. 43 - Loss: 0.9953528642654419. Acc: 0.5333333611488342\n",
      "Ep. 44 - Loss: 1.0009093284606934. Acc: 0.5333333611488342\n",
      "Ep. 45 - Loss: 0.9895124435424805. Acc: 0.5333333611488342\n",
      "Ep. 46 - Loss: 1.0011963844299316. Acc: 0.5333333611488342\n",
      "Ep. 47 - Loss: 1.0003087520599365. Acc: 0.5333333611488342\n",
      "Ep. 48 - Loss: 1.0061568021774292. Acc: 0.5333333611488342\n",
      "Ep. 49 - Loss: 1.000699520111084. Acc: 0.5333333611488342\n",
      "Ep. 50 - Loss: 1.002701997756958. Acc: 0.5333333611488342\n",
      "Ep. 51 - Loss: 1.0064188241958618. Acc: 0.5333333611488342\n",
      "Ep. 52 - Loss: 0.9982064366340637. Acc: 0.5333333611488342\n",
      "Ep. 53 - Loss: 1.0011156797409058. Acc: 0.5333333611488342\n",
      "Ep. 54 - Loss: 1.0061984062194824. Acc: 0.5333333611488342\n",
      "Ep. 55 - Loss: 1.0051262378692627. Acc: 0.5333333611488342\n",
      "Ep. 56 - Loss: 0.997088611125946. Acc: 0.5333333611488342\n",
      "Ep. 57 - Loss: 1.0018317699432373. Acc: 0.5333333611488342\n",
      "Ep. 58 - Loss: 1.0033186674118042. Acc: 0.5333333611488342\n",
      "Ep. 59 - Loss: 1.002228856086731. Acc: 0.5333333611488342\n",
      "Ep. 60 - Loss: 1.0001277923583984. Acc: 0.5333333611488342\n",
      "Ep. 61 - Loss: 0.9993237853050232. Acc: 0.5333333611488342\n",
      "Ep. 62 - Loss: 1.0018565654754639. Acc: 0.5333333611488342\n",
      "Ep. 63 - Loss: 0.9983049631118774. Acc: 0.5333333611488342\n",
      "Ep. 64 - Loss: 0.9981717467308044. Acc: 0.5333333611488342\n",
      "Ep. 65 - Loss: 1.0011711120605469. Acc: 0.5333333611488342\n",
      "Ep. 66 - Loss: 0.9972524642944336. Acc: 0.5333333611488342\n",
      "Ep. 67 - Loss: 1.0010662078857422. Acc: 0.5333333611488342\n",
      "Ep. 68 - Loss: 0.9970917105674744. Acc: 0.5333333611488342\n",
      "Ep. 69 - Loss: 0.9971091747283936. Acc: 0.5333333611488342\n",
      "Ep. 70 - Loss: 1.000927209854126. Acc: 0.5333333611488342\n",
      "Ep. 71 - Loss: 0.9972756505012512. Acc: 0.5333333611488342\n",
      "Ep. 72 - Loss: 0.9957275390625. Acc: 0.5333333611488342\n",
      "Ep. 73 - Loss: 0.997150719165802. Acc: 0.5333333611488342\n",
      "Ep. 74 - Loss: 0.9938730001449585. Acc: 0.5333333611488342\n",
      "Ep. 75 - Loss: 0.9964672327041626. Acc: 0.5333333611488342\n",
      "Ep. 76 - Loss: 0.9984456896781921. Acc: 0.5333333611488342\n",
      "Ep. 77 - Loss: 0.9996362328529358. Acc: 0.5333333611488342\n",
      "Ep. 78 - Loss: 0.9966866374015808. Acc: 0.5333333611488342\n",
      "Ep. 79 - Loss: 0.9985429644584656. Acc: 0.5333333611488342\n",
      "Ep. 80 - Loss: 1.004003643989563. Acc: 0.5333333611488342\n",
      "Ep. 81 - Loss: 1.0022214651107788. Acc: 0.5333333611488342\n",
      "Ep. 82 - Loss: 0.9963884353637695. Acc: 0.5333333611488342\n",
      "Ep. 83 - Loss: 1.0009737014770508. Acc: 0.5333333611488342\n",
      "Ep. 84 - Loss: 0.9982374906539917. Acc: 0.5333333611488342\n",
      "Ep. 85 - Loss: 1.000149130821228. Acc: 0.5333333611488342\n",
      "Ep. 86 - Loss: 0.9979526400566101. Acc: 0.5333333611488342\n",
      "Ep. 87 - Loss: 0.9966283440589905. Acc: 0.5333333611488342\n",
      "Ep. 88 - Loss: 0.9954677224159241. Acc: 0.5333333611488342\n",
      "Ep. 89 - Loss: 1.0026270151138306. Acc: 0.5333333611488342\n",
      "Ep. 90 - Loss: 1.0017762184143066. Acc: 0.5333333611488342\n",
      "Ep. 91 - Loss: 1.0003552436828613. Acc: 0.5333333611488342\n",
      "Ep. 92 - Loss: 1.0035961866378784. Acc: 0.5333333611488342\n",
      "Ep. 93 - Loss: 1.0019384622573853. Acc: 0.5333333611488342\n",
      "Ep. 94 - Loss: 0.9955931305885315. Acc: 0.5333333611488342\n",
      "Ep. 95 - Loss: 0.9949129819869995. Acc: 0.5333333611488342\n",
      "Ep. 96 - Loss: 0.9954840540885925. Acc: 0.5333333611488342\n",
      "Ep. 97 - Loss: 0.9973720908164978. Acc: 0.5333333611488342\n",
      "Ep. 98 - Loss: 0.9982039928436279. Acc: 0.5333333611488342\n",
      "Ep. 99 - Loss: 0.994094967842102. Acc: 0.5333333611488342\n",
      "Ep. 100 - Loss: 0.998746931552887. Acc: 0.5333333611488342\n",
      "Ep. 101 - Loss: 0.9982554912567139. Acc: 0.5333333611488342\n",
      "Ep. 102 - Loss: 0.999026358127594. Acc: 0.5333333611488342\n",
      "Ep. 103 - Loss: 1.0018723011016846. Acc: 0.5333333611488342\n",
      "Ep. 104 - Loss: 0.9990301728248596. Acc: 0.5333333611488342\n",
      "Ep. 105 - Loss: 0.9999247193336487. Acc: 0.5333333611488342\n",
      "Ep. 106 - Loss: 0.9953933954238892. Acc: 0.5333333611488342\n",
      "Ep. 107 - Loss: 0.9993177652359009. Acc: 0.5333333611488342\n",
      "Ep. 108 - Loss: 0.9981382489204407. Acc: 0.5333333611488342\n",
      "Ep. 109 - Loss: 0.9989069700241089. Acc: 0.5333333611488342\n",
      "Ep. 110 - Loss: 0.9964954257011414. Acc: 0.5333333611488342\n",
      "Ep. 111 - Loss: 0.9973188638687134. Acc: 0.5333333611488342\n",
      "Ep. 112 - Loss: 1.0003159046173096. Acc: 0.5333333611488342\n",
      "Ep. 113 - Loss: 0.9949048757553101. Acc: 0.5333333611488342\n",
      "Ep. 114 - Loss: 1.0004761219024658. Acc: 0.5333333611488342\n",
      "Ep. 115 - Loss: 0.9991511702537537. Acc: 0.5333333611488342\n",
      "Ep. 116 - Loss: 0.9971100687980652. Acc: 0.5333333611488342\n",
      "Ep. 117 - Loss: 0.9955059885978699. Acc: 0.5333333611488342\n",
      "Ep. 118 - Loss: 0.9987456202507019. Acc: 0.5333333611488342\n",
      "Ep. 119 - Loss: 0.9942805767059326. Acc: 0.5333333611488342\n",
      "Ep. 120 - Loss: 0.9962019324302673. Acc: 0.5333333611488342\n",
      "Ep. 121 - Loss: 1.000109076499939. Acc: 0.5333333611488342\n",
      "Ep. 122 - Loss: 0.9967153668403625. Acc: 0.5333333611488342\n",
      "Ep. 123 - Loss: 0.9945231676101685. Acc: 0.5333333611488342\n",
      "Ep. 124 - Loss: 0.9962171912193298. Acc: 0.5333333611488342\n",
      "Ep. 125 - Loss: 0.9967905879020691. Acc: 0.5333333611488342\n",
      "Ep. 126 - Loss: 0.9964079260826111. Acc: 0.5333333611488342\n",
      "Ep. 127 - Loss: 0.9983565807342529. Acc: 0.5333333611488342\n",
      "Ep. 128 - Loss: 0.996610701084137. Acc: 0.5333333611488342\n",
      "Ep. 129 - Loss: 0.9975970387458801. Acc: 0.5333333611488342\n",
      "Ep. 130 - Loss: 0.9968069791793823. Acc: 0.5333333611488342\n",
      "Ep. 131 - Loss: 0.9944486618041992. Acc: 0.5333333611488342\n",
      "Ep. 132 - Loss: 1.0024807453155518. Acc: 0.5333333611488342\n",
      "Ep. 133 - Loss: 0.9952693581581116. Acc: 0.5333333611488342\n",
      "Ep. 134 - Loss: 0.9994319677352905. Acc: 0.5333333611488342\n",
      "Ep. 135 - Loss: 1.0005474090576172. Acc: 0.5333333611488342\n",
      "Ep. 136 - Loss: 0.9952910542488098. Acc: 0.5333333611488342\n",
      "Ep. 137 - Loss: 0.9972249865531921. Acc: 0.5333333611488342\n",
      "Ep. 138 - Loss: 0.9949294328689575. Acc: 0.5333333611488342\n",
      "Ep. 139 - Loss: 0.9976814389228821. Acc: 0.5333333611488342\n",
      "Ep. 140 - Loss: 0.996368944644928. Acc: 0.5333333611488342\n",
      "Ep. 141 - Loss: 0.9959619045257568. Acc: 0.5333333611488342\n",
      "Ep. 142 - Loss: 0.9985147714614868. Acc: 0.5333333611488342\n",
      "Ep. 143 - Loss: 0.996590793132782. Acc: 0.5333333611488342\n",
      "Ep. 144 - Loss: 0.9974185824394226. Acc: 0.5333333611488342\n",
      "Ep. 145 - Loss: 1.001590609550476. Acc: 0.5333333611488342\n",
      "Ep. 146 - Loss: 0.995543360710144. Acc: 0.5333333611488342\n",
      "Ep. 147 - Loss: 0.9969209432601929. Acc: 0.5333333611488342\n",
      "Ep. 148 - Loss: 0.9960082769393921. Acc: 0.5333333611488342\n",
      "Ep. 149 - Loss: 0.9961411952972412. Acc: 0.5333333611488342\n",
      "Ep. 150 - Loss: 0.9963948130607605. Acc: 0.5333333611488342\n",
      "Ep. 151 - Loss: 0.9995020031929016. Acc: 0.5333333611488342\n",
      "Ep. 152 - Loss: 0.9966966509819031. Acc: 0.5333333611488342\n",
      "Ep. 153 - Loss: 0.9963517785072327. Acc: 0.5333333611488342\n",
      "Ep. 154 - Loss: 0.9962237477302551. Acc: 0.5333333611488342\n",
      "Ep. 155 - Loss: 0.9959442615509033. Acc: 0.5333333611488342\n",
      "Ep. 156 - Loss: 0.9960764646530151. Acc: 0.5333333611488342\n",
      "Ep. 157 - Loss: 0.9958574175834656. Acc: 0.5333333611488342\n",
      "Ep. 158 - Loss: 0.9958752393722534. Acc: 0.5333333611488342\n",
      "Ep. 159 - Loss: 0.9969754815101624. Acc: 0.5333333611488342\n",
      "Ep. 160 - Loss: 0.9968536496162415. Acc: 0.5333333611488342\n",
      "Ep. 161 - Loss: 0.9962714910507202. Acc: 0.5333333611488342\n",
      "Ep. 162 - Loss: 0.9966307282447815. Acc: 0.5333333611488342\n",
      "Ep. 163 - Loss: 0.9960304498672485. Acc: 0.5333333611488342\n",
      "Ep. 164 - Loss: 0.9977415800094604. Acc: 0.5333333611488342\n",
      "Ep. 165 - Loss: 0.9966647624969482. Acc: 0.5333333611488342\n",
      "Ep. 166 - Loss: 0.9956321716308594. Acc: 0.5333333611488342\n",
      "Ep. 167 - Loss: 0.9961494207382202. Acc: 0.5333333611488342\n",
      "Ep. 168 - Loss: 0.9967869520187378. Acc: 0.5333333611488342\n",
      "Ep. 169 - Loss: 0.998285174369812. Acc: 0.5333333611488342\n",
      "Ep. 170 - Loss: 0.9964572787284851. Acc: 0.5333333611488342\n",
      "Ep. 171 - Loss: 0.995984673500061. Acc: 0.5333333611488342\n",
      "Ep. 172 - Loss: 0.9960660934448242. Acc: 0.5333333611488342\n",
      "Ep. 173 - Loss: 0.9970630407333374. Acc: 0.5333333611488342\n",
      "Ep. 174 - Loss: 0.9957576990127563. Acc: 0.5333333611488342\n",
      "Ep. 175 - Loss: 0.996182382106781. Acc: 0.5333333611488342\n",
      "Ep. 176 - Loss: 0.9962243437767029. Acc: 0.5333333611488342\n",
      "Ep. 177 - Loss: 0.9958283305168152. Acc: 0.5333333611488342\n",
      "Ep. 178 - Loss: 0.995629608631134. Acc: 0.5333333611488342\n",
      "Ep. 179 - Loss: 0.9959362745285034. Acc: 0.5333333611488342\n",
      "Ep. 180 - Loss: 0.9957957863807678. Acc: 0.5333333611488342\n",
      "Ep. 181 - Loss: 0.9958264827728271. Acc: 0.5333333611488342\n",
      "Ep. 182 - Loss: 0.9956810474395752. Acc: 0.5333333611488342\n",
      "Ep. 183 - Loss: 0.9956727623939514. Acc: 0.5333333611488342\n",
      "Ep. 184 - Loss: 0.9958236813545227. Acc: 0.5333333611488342\n",
      "Ep. 185 - Loss: 0.9956847429275513. Acc: 0.5333333611488342\n",
      "Ep. 186 - Loss: 0.9957282543182373. Acc: 0.5333333611488342\n",
      "Ep. 187 - Loss: 0.9956766366958618. Acc: 0.5333333611488342\n",
      "Ep. 188 - Loss: 0.995664656162262. Acc: 0.5333333611488342\n",
      "Ep. 189 - Loss: 0.9956257343292236. Acc: 0.5333333611488342\n",
      "Ep. 190 - Loss: 0.9955701231956482. Acc: 0.5333333611488342\n",
      "Ep. 191 - Loss: 0.9959275722503662. Acc: 0.5333333611488342\n",
      "Ep. 192 - Loss: 0.9958509206771851. Acc: 0.5333333611488342\n",
      "Ep. 193 - Loss: 0.995815098285675. Acc: 0.5333333611488342\n",
      "Ep. 194 - Loss: 0.9954992532730103. Acc: 0.5333333611488342\n",
      "Ep. 195 - Loss: 0.9960060715675354. Acc: 0.5333333611488342\n",
      "Ep. 196 - Loss: 0.9957826137542725. Acc: 0.5333333611488342\n",
      "Ep. 197 - Loss: 0.9957531094551086. Acc: 0.5333333611488342\n",
      "Ep. 198 - Loss: 0.9957699179649353. Acc: 0.5333333611488342\n",
      "Ep. 199 - Loss: 0.9958152770996094. Acc: 0.5333333611488342\n",
      "Ep. 200 - Loss: 0.9956891536712646. Acc: 0.5333333611488342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold: 100%|██████████| 5/5 [47:52<00:00, 574.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Test loss: 0.9963248372077942. Test acc: 0.5317725539207458\n",
      "Done. Test loss: 0.9963248372077942. Test acc: 0.5317725539207458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataname' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a05838a2b473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m   \u001b[0mdfres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0mdfres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m dfres = dfres.append({'dataset': dataname,\n\u001b[0m\u001b[1;32m    154\u001b[0m                       \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                       \u001b[0;34m'acc'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataname' is not defined"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import tensorflow as tf\n",
    "import tqdm.notebook as tq\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.data import DisjointLoader\n",
    "from spektral.datasets import TUDataset\n",
    "from spektral.layers import GINConv, GlobalSumPool, GlobalAvgPool\n",
    "from spektral.data import DisjointLoader\n",
    "from spektral.datasets import TUDataset\n",
    "from spektral.layers import GINConv, GlobalSumPool, GlobalAvgPool\n",
    "\n",
    "import tqdm as tq\n",
    "\n",
    "################################################################################\n",
    "# Config\n",
    "################################################################################\n",
    "#@title Parameters { form-width: \"30%\" }\n",
    "learning_rate = 0.001  #@param {type:\"number\"}\n",
    "epochs = 200 #@param {type:\"slider\", min:0, max:500, step:20}\n",
    "channels = 64 #@param {type:\"slider\", min:16, max:128, step:16}\n",
    "batch_size = 1  #@param {type:\"slider\", min:1, max:64, step:1}\n",
    "layers = 3  #@param {type:\"slider\", min:1, max:5, step:1}\n",
    "folds = 5  #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "verbose = True #@param {type:\"boolean\"}\n",
    "seed = 42 #@param {type:\"number\"}\n",
    "\n",
    "# Parameters\n",
    "F = dataset.n_node_features  # Dimension of node features\n",
    "n_out = dataset.n_labels  # Dimension of the target\n",
    "print(\"Node features\", F)\n",
    "print(dataset[0].x)\n",
    "#sys.exit(0)\n",
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "class GIN0(Model):\n",
    "    def __init__(self, channels, n_layers):\n",
    "        super().__init__()\n",
    "        self.conv1 = GINConv(channels, epsilon=0, mlp_hidden=[channels, channels])\n",
    "        self.convs = []\n",
    "        for _ in range(1, n_layers):\n",
    "            self.convs.append(\n",
    "                GINConv(channels, epsilon=0, mlp_hidden=[channels, channels])\n",
    "            )\n",
    "        self.pool = GlobalAvgPool()\n",
    "        self.dense1 = Dense(channels, activation=\"relu\")\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense2 = Dense(n_out, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, a, = inputs[0:2]\n",
    "        i = inputs[-1]\n",
    "        #x, a, i = inputs\n",
    "        x = self.conv1([x, a])\n",
    "        for conv in self.convs:\n",
    "            x = conv([x, a])\n",
    "        x = self.pool([x, i])\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "start = time()\n",
    "\n",
    "# Cross Validation loop\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "targets = [g.y.dot(1 << np.arange(g.y.size)[::-1]) for g in dataset]\n",
    "sp = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "results = []\n",
    "for idx_tr, idx_te in tq.tqdm(list(sp.split(dataset, targets)), desc=\"fold: \"):\n",
    "  dataset_tr, dataset_te = dataset[idx_tr], dataset[idx_te]\n",
    "\n",
    "  loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
    "  loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1)\n",
    "\n",
    "################################################################################\n",
    "# Fit model\n",
    "################################################################################\n",
    "  @tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "  def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "    return loss, acc\n",
    "    \n",
    "  # Build model\n",
    "  model = GIN0(channels, layers)\n",
    "  optimizer = Adam(learning_rate)\n",
    "  loss_fn = CategoricalCrossentropy()\n",
    "  @tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "  def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "    return loss, acc\n",
    "  # Build model\n",
    "  model = GIN0(channels, layers)\n",
    "  optimizer = Adam(learning_rate)\n",
    "  loss_fn = CategoricalCrossentropy()\n",
    "\n",
    "  epoch = step = 0\n",
    "  tresults = []\n",
    "  for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss, acc = train_step(*batch)\n",
    "    tresults.append((loss, acc))\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "        if verbose: print(\"Ep. {} - Loss: {}. Acc: {}\".format(epoch, *np.mean(tresults, 0)))\n",
    "        tresults = []\n",
    "\n",
    "################################################################################\n",
    "# Evaluate model\n",
    "################################################################################\n",
    "  for batch in loader_te:\n",
    "    inputs, target = batch\n",
    "    predictions = model(inputs, training=False)\n",
    "    results.append(\n",
    "        (\n",
    "            loss_fn(target, predictions),\n",
    "            tf.reduce_mean(categorical_accuracy(target, predictions)),\n",
    "        )\n",
    "    )\n",
    "  if verbose: print(\"Done. Test loss: {}. Test acc: {}\".format(*np.mean(results, 0)))\n",
    "# Timing\n",
    "temp = time() - start\n",
    "hours = temp//3600\n",
    "temp = temp - 3600*hours\n",
    "minutes = temp//60\n",
    "seconds = temp - 60*minutes\n",
    "expired = '%d:%d:%d' %(hours,minutes,seconds)\n",
    "print(\"Done. Test loss: {}. Test acc: {}\".format(*np.mean(results, 0)))\n",
    "\n",
    "method = 'GIN'\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "path = f'{method}_results.csv'\n",
    "if not os.path.exists(path):\n",
    "  dfres = pd.DataFrame(columns=['dataset', 'loss', 'acc', 'folds', 'seed', 'lr', 'epochs', 'batch_size', 'layers', 'channels', 'date', 'elapsed'])\n",
    "  dfres.to_csv(path, index=False)\n",
    "dfres = pd.read_csv(path)\n",
    "dfres = dfres.append({'dataset': dataname,\n",
    "                      'loss': np.mean(results, 0)[0],\n",
    "                      'acc' : np.mean(results, 0)[1],\n",
    "                      'folds' : folds,\n",
    "                      'seed' : seed,\n",
    "                      'lr' : learning_rate, 'epochs' : epochs, 'batch_size' : batch_size, 'layers': layers, 'channels' : channels,\n",
    "                      'date': datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"),\n",
    "                      'elapsed': expired\n",
    "                      }, ignore_index=True)\n",
    "dfres.to_csv(path, index=False)\n",
    "dfres"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
